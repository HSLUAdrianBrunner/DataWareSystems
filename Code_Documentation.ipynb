{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLjx451v5-xg"
   },
   "source": [
    "# Code Documentation of Group2 | Datalake and Datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUO319FO6Xnk"
   },
   "source": [
    "## Lamdba Functions for API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6UHp_Zh6sfj"
   },
   "source": [
    "### group2_currency_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE1GRu4j6YOw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Get Environment Variables\n",
    "\n",
    "endpoint = os.getenv('ENDPOINT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "username = os.getenv('USERNAME')\n",
    "password = os.getenv('PASSWORD')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n",
    "\n",
    "\n",
    "def upload_dataframe_to_db(df, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Replace invalid characters in column names and convert index to a column\n",
    "    df = df.copy()\n",
    "    df.index.name = 'timestamp'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "\n",
    "    # Generate CREATE TABLE query\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col in ['unit', 'country'] else 'float')}\"\n",
    "        for col in df.columns\n",
    "    ])\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    # INSERT query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Convert NaN to None for SQL compatibility\n",
    "    values = [tuple(None if pd.isna(x) else x for x in row) for row in df.values]\n",
    "    cur.executemany(insert_query, values)\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "def upload_dataframe_to_bucket(df, foldername,s3 ,bucket_name):\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = foldername + '/' + foldername + '_' + today + '.json'  # Key = path in the bucket\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def fetch_cbet_data(date: str) -> dict:\n",
    "    base_url = \"https://api.energy-charts.info/cbet\"\n",
    "    params = {\n",
    "        \"country\": \"ch\",\n",
    "        \"start\": date\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error during requesting CBET-Data for {date}: {response.status_code}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_cbet_data(json_data: dict) -> pd.DataFrame:\n",
    "    idx = pd.to_datetime(json_data[\"unix_seconds\"], unit=\"s\", utc=True)\n",
    "    idx.name = \"timestamp\"\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for country in json_data.get(\"countries\", []):\n",
    "        values = country.get(\"data\", [])\n",
    "        if len(values) != len(idx):\n",
    "            # length guard â€“ optional but helpful for debugging mismatches\n",
    "            raise ValueError(\n",
    "                f\"Length mismatch for {country.get('name')}: \"\n",
    "                f\"{len(values)} values vs {len(idx)} timestamps\"\n",
    "            )\n",
    "        df = pd.DataFrame(\n",
    "            {\"value\": values, \"country\": country.get(\"name\")},\n",
    "            index=idx\n",
    "        )\n",
    "        frames.append(df)\n",
    "\n",
    "    return pd.concat(frames).reset_index()\n",
    "\n",
    "def insert_cbet_data_to_db(df_cbet, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    df_cbet = df_cbet.fillna(0)\n",
    "\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col in ['unit', 'country'] else 'float')}\"\n",
    "        for col in df_cbet.columns\n",
    "        ])\n",
    "\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (timestamp, country, value)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    data_tuples = [\n",
    "        (row['timestamp'], row['country'], row['value'])\n",
    "        for _, row in df_cbet.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"CBET-Data successfully implemented.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in implementing CBET-Data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    date = dt.datetime.today().date()\n",
    "    start_date = str(date-dt.timedelta(days=1))\n",
    "\n",
    "    api_result_power = requests.get(f'https://api.energy-charts.info/public_power?country=ch&start={start_date}')\n",
    "    api_result_prices = requests.get(f'https://api.energy-charts.info/price?bzn=CH&start={start_date}')\n",
    "\n",
    "    api_response_power = api_result_power.json()\n",
    "    api_response_prices = api_result_prices.json()\n",
    "\n",
    "    cbet_json = fetch_cbet_data(start_date)\n",
    "\n",
    "    api_df_power = pd.DataFrame(api_response_power[\"production_types\"])\n",
    "    api_df_power_t = pd.DataFrame(api_df_power['data'].tolist()).T\n",
    "    api_df_power_t.columns = api_df_power['name']\n",
    "    api_df_power_t.index = pd.to_datetime(api_response_power[\"unix_seconds\"], unit='s', utc=True)\n",
    "\n",
    "    api_df_price = pd.DataFrame(api_response_prices[\"price\"])\n",
    "    api_df_price['Unit'] = api_response_prices[\"unit\"]\n",
    "    api_df_price.columns = ['Price', 'Unit']\n",
    "    api_df_price.index = pd.to_datetime(api_response_prices[\"unix_seconds\"], unit='s', utc=True)\n",
    "\n",
    "    api_df_cbet = extract_cbet_data(cbet_json)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client('s3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "\n",
    "        # Upload DataFrames\n",
    "        upload_dataframe_to_db(api_df_power_t, \"tbl_energy_production_data\", conn)\n",
    "        upload_dataframe_to_db(api_df_price, \"tbl_energy_price_data\", conn)\n",
    "        insert_cbet_data_to_db(api_df_cbet, \"tbl_energy_cbet_data\", conn)\n",
    "\n",
    "\n",
    "\n",
    "        upload_dataframe_to_bucket(api_df_power_t, \"energy_production\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_price, \"energy_price\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_cbet, \"energy_cbet\",s3,bucket_name)\n",
    "\n",
    "        conn.close()\n",
    "        print(\"Data uploaded and connection closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(\"Inport succesfully\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3WvVZQo7uZ2"
   },
   "source": [
    "### group2_energy_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdEhtA657ThA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Get Environment Variables\n",
    "\n",
    "endpoint = os.getenv('ENDPOINT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "username = os.getenv('USERNAME')\n",
    "password = os.getenv('PASSWORD')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n",
    "\n",
    "\n",
    "def upload_dataframe_to_db(df, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Replace invalid characters in column names and convert index to a column\n",
    "    df = df.copy()\n",
    "    df.index.name = 'timestamp'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "\n",
    "    # Generate CREATE TABLE query\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'unit' else 'float')}\"\n",
    "        for col in df.columns\n",
    "    ])\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    # INSERT query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Convert NaN to None for SQL compatibility\n",
    "    values = [tuple(None if pd.isna(x) else x for x in row) for row in df.values]\n",
    "    cur.executemany(insert_query, values)\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "def upload_dataframe_to_bucket(df, foldername,s3 ,bucket_name):\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = foldername + '/' + foldername + '_' + today + '.json'  # Key = path in the bucket\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def fetch_cbet_data(date: str) -> dict:\n",
    "    base_url = \"https://api.energy-charts.info/cbet\"\n",
    "    params = {\n",
    "        \"country\": \"ch\",\n",
    "        \"start\": date\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error during requesting CBET-Data for {date}: {response.status_code}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_cbet_data(json_data: dict) -> pd.DataFrame:\n",
    "    records = []\n",
    "    timestamps = json_data.get('unix_seconds', [])\n",
    "    for country in json_data.get('countries', []):\n",
    "        country_name = country.get('name')\n",
    "        values = country.get('data', [])\n",
    "\n",
    "        for ts, value in zip(timestamps, values):\n",
    "            dtime = dt.datetime.utcfromtimestamp(ts)\n",
    "            records.append({\n",
    "                'timestamp': dtime,\n",
    "                'country': country_name,\n",
    "                'value': value\n",
    "            })\n",
    "            df_records = pd.DataFrame(records)\n",
    "    return df_records\n",
    "\n",
    "def insert_cbet_data_to_db(df_cbet, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    df_cbet = df_cbet.fillna(0)\n",
    "\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'country' else 'float')}\"\n",
    "        for col in df_cbet.columns\n",
    "        ])\n",
    "\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (timestamp, country, value)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    data_tuples = [\n",
    "        (row['timestamp'], row['country'], row['value'])\n",
    "        for _, row in df_cbet.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"CBET-Data successfully implemented.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in implementing CBET-Data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    date = dt.datetime.today().date()\n",
    "    start_date = str(date-dt.timedelta(days=1))\n",
    "\n",
    "    api_result_power = requests.get(f'https://api.energy-charts.info/public_power?country=ch&start={start_date}')\n",
    "    api_result_prices = requests.get(f'https://api.energy-charts.info/price?bzn=CH&start={start_date}')\n",
    "\n",
    "    api_response_power = api_result_power.json()\n",
    "    api_response_prices = api_result_prices.json()\n",
    "\n",
    "    cbet_json = fetch_cbet_data(start_date)\n",
    "\n",
    "    api_df_power = pd.DataFrame(api_response_power[\"production_types\"])\n",
    "    api_df_power_t = pd.DataFrame(api_df_power['data'].tolist()).T\n",
    "    api_df_power_t.columns = api_df_power['name']\n",
    "    api_df_power_t.index = pd.to_datetime(api_response_power[\"unix_seconds\"], unit='s')\n",
    "\n",
    "    api_df_price = pd.DataFrame(api_response_prices[\"price\"])\n",
    "    api_df_price['Unit'] = api_response_prices[\"unit\"]\n",
    "    api_df_price.columns = ['Price', 'Unit']\n",
    "    api_df_price.index = pd.to_datetime(api_response_prices[\"unix_seconds\"], unit='s')\n",
    "\n",
    "    api_df_cbet = extract_cbet_data(cbet_json)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client('s3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "\n",
    "        # Upload DataFrames\n",
    "        upload_dataframe_to_db(api_df_power_t, \"tbl_energy_production_data\", conn)\n",
    "        upload_dataframe_to_db(api_df_price, \"tbl_energy_price_data\", conn)\n",
    "        insert_cbet_data_to_db(api_df_cbet, \"tbl_energy_cbet_data\", conn)\n",
    "\n",
    "\n",
    "\n",
    "        upload_dataframe_to_bucket(api_df_power_t, \"energy_production\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_price, \"energy_price\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_cbet, \"energy_cbet\",s3,bucket_name)\n",
    "\n",
    "        conn.close()\n",
    "        print(\"Data uploaded and connection closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(\"Inport succesfully\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gpQwKYX7eVs"
   },
   "source": [
    "### group2_weather_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXJWAjAB7UGv"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import boto3\n",
    "import os\n",
    "# Env Variable\n",
    "var_apikey = os.getenv(\"var_apikey\")\n",
    "var_database = os.getenv(\"var_database\")\n",
    "var_database2 = os.getenv(\"var_database2\")\n",
    "var_host = os.getenv(\"var_host\")\n",
    "var_host2 = os.getenv(\"var_host2\")\n",
    "var_password = os.getenv(\"var_password\")\n",
    "var_password2 = os.getenv(\"var_password2\")\n",
    "var_port = os.getenv(\"var_port\")\n",
    "var_user = os.getenv(\"var_user\")\n",
    "var_bucketname = os.getenv(\"var_bucketname\")\n",
    "var_aws_access_key_id = os.getenv(\"var_aws_access_key_id\")\n",
    "var_aws_secret_access_key = os.getenv(\"var_aws_secret_access_key\")\n",
    "\n",
    "# === PARAMETER ============================================\n",
    "DAYS_BACK = 1  # defining the amount of days from yesterday backwards\n",
    "\n",
    "# configuration parameter for db and bucket\n",
    "BUCKET_NAME = var_bucketname\n",
    "DB_CONFIG = {\n",
    "    \"host\": var_host,\n",
    "    \"port\": var_port,\n",
    "    \"database\": var_database,\n",
    "    \"user\": var_user,\n",
    "    \"password\": var_password\n",
    "}\n",
    "DB_CONFIG2 = {\n",
    "    \"host\": os.getenv(\"var_host2\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database2\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password2\")\n",
    "}\n",
    "API_KEY = var_apikey\n",
    "AWS_ACCESS_KEY_ID = var_aws_access_key_id\n",
    "AWS_SECRET_ACCESS_KEY = var_aws_secret_access_key\n",
    "\n",
    "# === FUNCTIONS ==============================================\n",
    "#Connecting to database\n",
    "def connect_to_db():\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "def connect_to_db2():\n",
    "    return psycopg2.connect(**DB_CONFIG2)\n",
    "\n",
    "#Reading in location data from dim_locations\n",
    "def get_city_data():\n",
    "    conn = connect_to_db2()\n",
    "    try:\n",
    "        query = \"SELECT city, lat, long FROM dim_locations;\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "#Getting data from API\n",
    "def fetch_weather_data(city, lat, lon, start, end, api_key):\n",
    "    url = f\"https://history.openweathermap.org/data/2.5/history/city?lat={lat}&lon={lon}&type=hour&start={start}&end={end}&appid={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error on {city}: {response.status_code}\")\n",
    "        return None\n",
    "    return response.json()\n",
    "\n",
    "#Loading data into AWS bucket\n",
    "def upload_to_s3(s3_client, city, date_str, data):\n",
    "    object_key = f\"weather_data/weatherdata_{city.lower()}_{date_str}.json\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=object_key,\n",
    "        Body=json.dumps(data, ensure_ascii=False).encode('utf-8'),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "#Extracting data from recieved json of API\n",
    "\n",
    "def extract_weather_data(city, weather_json):\n",
    "    entries = []\n",
    "    for entry in weather_json.get(\"list\", []):\n",
    "        dt = datetime.utcfromtimestamp(entry[\"dt\"])\n",
    "        main = entry[\"main\"]\n",
    "        wind = entry.get(\"wind\", {})\n",
    "        weather = entry[\"weather\"][0]\n",
    "        rain = entry.get(\"rain\", {}).get(\"1h\", 0.0)\n",
    "        clouds = entry.get(\"clouds\", {}).get(\"all\", 0)\n",
    "        entries.append({\n",
    "            'City': city,\n",
    "            'datetime': dt,\n",
    "            'temp': main['temp'],\n",
    "            'pressure': main['pressure'],\n",
    "            'humidity': main['humidity'],\n",
    "            'temp_min': main['temp_min'],\n",
    "            'temp_max': main['temp_max'],\n",
    "            'wind_speed': wind.get('speed', 0.0),\n",
    "            'weather_main': weather['main'],\n",
    "            'weather_description': weather['description'],\n",
    "            'rain_1h': rain,\n",
    "            'clouds_all': clouds\n",
    "        })\n",
    "    return entries\n",
    "\n",
    "#writing data into datalake database\n",
    "def insert_weather_data(conn, df_weather):\n",
    "    cur = conn.cursor()\n",
    "    df_weather = df_weather.fillna(0)\n",
    "    data_tuples = [\n",
    "        (\n",
    "            row['City'],\n",
    "            row['datetime'],\n",
    "            row['temp'],\n",
    "            row['pressure'],\n",
    "            row['humidity'],\n",
    "            row['temp_min'],\n",
    "            row['temp_max'],\n",
    "            row['wind_speed'],\n",
    "            row['weather_main'],\n",
    "            row['weather_description'],\n",
    "            row['rain_1h'],\n",
    "            row['clouds_all']\n",
    "        )\n",
    "        for _, row in df_weather.iterrows()\n",
    "    ]\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO tbl_weather_data (\n",
    "        city, datetime, temp, pressure, humidity,\n",
    "        temp_min, temp_max, wind_speed,\n",
    "        weather_main, weather_description, rain_1h, clouds_all\n",
    "    ) VALUES %s\n",
    "    \"\"\"\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inserting data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "# === MAIN LAMBDA HANDLER ====================================\n",
    "def lambda_handler(event, context):\n",
    "    #-> defined function <get_city_data>\n",
    "    df_coords = get_city_data()\n",
    "\n",
    "    #Opening bucket connection\n",
    "    s3 = boto3.client('s3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    END_DATE = datetime.utcnow() - timedelta(days=1)\n",
    "    all_data = []\n",
    "\n",
    "    #iterating through day requests of the past (usual 1 for yesterday -> manually adaptable for historical data)\n",
    "    for days_ago in range(DAYS_BACK):\n",
    "        target_day = END_DATE - timedelta(days=days_ago)\n",
    "        start = int(datetime(target_day.year, target_day.month, target_day.day, 0, 0).timestamp())\n",
    "        end = int(datetime(target_day.year, target_day.month, target_day.day, 23, 59).timestamp())\n",
    "        date_str = target_day.strftime('%Y%m%d')\n",
    "\n",
    "        #iterating through weather station locations (location data from dim_locations)\n",
    "        for _, row in df_coords.iterrows():\n",
    "            city = row[\"city\"]\n",
    "            lat = row[\"lat\"]\n",
    "            lon = row[\"long\"]\n",
    "\n",
    "            #-> defined function <fetch_weather_data()>\n",
    "            weather_json = fetch_weather_data(city, lat, lon, start, end, API_KEY)\n",
    "            if not weather_json:\n",
    "                continue\n",
    "\n",
    "            #-> defined function <upload_to_s3>\n",
    "            upload_to_s3(s3, city, date_str, weather_json)\n",
    "\n",
    "            #-> defined function <extract_weather_data()>\n",
    "            city_data = extract_weather_data(city, weather_json)\n",
    "            all_data.extend(city_data)\n",
    "    df_weather = pd.DataFrame(all_data)\n",
    "\n",
    "    #Inserting data into datalake database\n",
    "    if not df_weather.empty:\n",
    "        try:\n",
    "            conn = connect_to_db()\n",
    "\n",
    "            #-> defined function <inser_weather_data>\n",
    "            insert_weather_data(conn, df_weather)\n",
    "        except Exception as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    else:\n",
    "        print(\"No weather data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### group2_weather_sun_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "from zoneinfo import ZoneInfo\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# Load environment variables for DB and AWS credentials\n",
    "endpoint: str = os.getenv('ENDPOINT')\n",
    "db_name: str = os.getenv('DB_NAME')\n",
    "username: str = os.getenv('USERNAME')\n",
    "password: str = os.getenv('PASSWORD')\n",
    "aws_access_key_id: str = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key: str = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name: str = os.getenv('BUCKET_NAME')\n",
    "\n",
    "# Secondary DB config dictionary\n",
    "DB_CONFIG2: Dict[str, Optional[str]] = {\n",
    "    \"host\": os.getenv(\"var_host2\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database2\"),\n",
    "    \"user\": os.getenv(\"USERNAME\"),\n",
    "    \"password\": os.getenv(\"var_password2\")\n",
    "}\n",
    "\n",
    "# Upload a DataFrame to a PostgreSQL table with upsert logic\n",
    "def upload_dataframe_to_db(df: pd.DataFrame, table_name: str, conn: psycopg2.extensions.connection) -> None:\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # SQL to create table if it does not exist\n",
    "    CREATE_TABLE_SQL = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        city         text          NOT NULL,\n",
    "        date         date          NOT NULL,\n",
    "        sunrise      timestamp     NOT NULL,\n",
    "        sunset       timestamp     NOT NULL,\n",
    "        first_light  timestamp     NOT NULL,\n",
    "        last_light   timestamp     NOT NULL,\n",
    "        dawn         timestamp     NOT NULL,\n",
    "        dusk         timestamp     NOT NULL,\n",
    "        solar_noon   timestamp     NOT NULL,\n",
    "        golden_hour  timestamp     NOT NULL,\n",
    "        day_length   interval      NOT NULL,\n",
    "        PRIMARY KEY (city, date)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.execute(CREATE_TABLE_SQL)\n",
    "\n",
    "    # UPSERT statement using ON CONFLICT\n",
    "    INSERT_SQL = f\"\"\"\n",
    "    INSERT INTO {table_name} (\n",
    "        city, date, sunrise, sunset, first_light, last_light,\n",
    "        dawn, dusk, solar_noon, golden_hour, day_length\n",
    "    ) VALUES %s\n",
    "    ON CONFLICT (city, date) DO UPDATE SET\n",
    "        sunrise      = EXCLUDED.sunrise,\n",
    "        sunset       = EXCLUDED.sunset,\n",
    "        first_light  = EXCLUDED.first_light,\n",
    "        last_light   = EXCLUDED.last_light,\n",
    "        dawn         = EXCLUDED.dawn,\n",
    "        dusk         = EXCLUDED.dusk,\n",
    "        solar_noon   = EXCLUDED.solar_noon,\n",
    "        golden_hour  = EXCLUDED.golden_hour,\n",
    "        day_length   = EXCLUDED.day_length;\n",
    "    \"\"\"\n",
    "\n",
    "    cols = [\n",
    "        \"city\", \"date\", \"sunrise\", \"sunset\", \"first_light\", \"last_light\",\n",
    "        \"dawn\", \"dusk\", \"solar_noon\", \"golden_hour\", \"day_length\"\n",
    "    ]\n",
    "    rows = df[cols].where(pd.notnull(df), None).to_numpy().tolist()\n",
    "\n",
    "    execute_values(cur, INSERT_SQL, rows, page_size=500)\n",
    "    cur.close()\n",
    "\n",
    "# Upload DataFrame to S3 bucket as JSON\n",
    "def upload_dataframe_to_bucket(df: pd.DataFrame, foldername: str, s3: Any, bucket_name: str) -> None:\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = f\"{foldername}/{foldername}_{today}.json\"  # S3 key/path\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "# Connect to secondary PostgreSQL DB\n",
    "def connect_to_db2() -> psycopg2.extensions.connection:\n",
    "    return psycopg2.connect(**DB_CONFIG2)\n",
    "\n",
    "# Get city name and coordinates from `dim_locations`\n",
    "def get_city_data() -> pd.DataFrame:\n",
    "    conn = connect_to_db2()\n",
    "    try:\n",
    "        query = \"SELECT city, lat, long FROM dim_locations;\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        return df\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Fetch sunrise/sunset JSON data for one city\n",
    "def fetch_sun_data(city: str, lat: float, lng: float, start: str) -> Optional[Dict[str, Any]]:\n",
    "    url = f\"https://api.sunrisesunset.io/json?lat={lat}&lng={lng}&date={start}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error on {city}: {response.status_code}\")\n",
    "        return None\n",
    "    return response.json()\n",
    "\n",
    "# Normalize JSON API result into a single-row DataFrame\n",
    "def extract_sun_data(city: str, sun_json: Dict[str, Any]) -> pd.DataFrame:\n",
    "    df = pd.json_normalize(sun_json[\"results\"])\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    df[\"city\"] = city\n",
    "    return df\n",
    "\n",
    "# Lambda entry point for AWS execution\n",
    "def lambda_handler(event: dict, context: Any) -> Dict[str, Any]:\n",
    "    # Step 1: Retrieve city data (name + coordinates)\n",
    "    df_coords = get_city_data()\n",
    "    all_sun_data = pd.DataFrame([])\n",
    "\n",
    "    # Step 2: Loop through each city and request yesterdayâ€™s sun data\n",
    "    for _, row in df_coords.iterrows():\n",
    "        city = row[\"city\"]\n",
    "        lat = row[\"lat\"]\n",
    "        lng = row[\"long\"]\n",
    "        sun_json = fetch_sun_data(city, lat, lng, start=str(dt.datetime.now().date() - dt.timedelta(days=1)))\n",
    "        if sun_json:\n",
    "            all_sun_data = pd.concat([all_sun_data, extract_sun_data(city, sun_json)], ignore_index=True)\n",
    "\n",
    "    # Step 3: Convert time columns to UTC timezone-aware timestamps\n",
    "    time_cols = [\n",
    "        \"sunrise\", \"sunset\", \"first_light\", \"last_light\",\n",
    "        \"dawn\", \"dusk\", \"solar_noon\", \"golden_hour\"\n",
    "    ]\n",
    "\n",
    "    for col in time_cols:\n",
    "        all_sun_data[col] = pd.to_datetime(\n",
    "            all_sun_data[\"date\"].dt.strftime(\"%Y-%m-%d\") + \" \" + all_sun_data[col],\n",
    "            format=\"%Y-%m-%d %I:%M:%S %p\"\n",
    "        ).dt.tz_localize(ZoneInfo(\"UTC\"))\n",
    "\n",
    "    # Step 4: Convert duration string to timedelta\n",
    "    all_sun_data[\"day_length\"] = pd.to_timedelta(all_sun_data[\"day_length\"])\n",
    "\n",
    "    # Step 5: Connect to PostgreSQL and S3\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client(\n",
    "            's3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "        )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "\n",
    "        # Step 6: Upload to PostgreSQL\n",
    "        upload_dataframe_to_db(all_sun_data, \"tbl_weather_sun_data\", conn)\n",
    "        print(\"Data uploaded to DB successfully.\")\n",
    "\n",
    "        # Step 7: Upload to S3 bucket\n",
    "        upload_dataframe_to_bucket(all_sun_data, \"weather_sun\", s3, bucket_name)\n",
    "        print(\"Data uploaded to Bucket successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group2_currency_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import psycopg2\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load sensitive configuration values from environment variables.\n",
    "# This keeps credentials and connection details out of the codebase.\n",
    "ENDPOINT = os.environ['ENDPOINT']\n",
    "DB_NAME = os.environ['DB_NAME']\n",
    "USERNAME = os.environ['USERNAME']\n",
    "PASSWORD = os.environ['PASSWORD']\n",
    "S3_BUCKET = os.environ['S3_BUCKET'] \n",
    "API_KEY = os.environ['API_KEY']\n",
    "\n",
    "def fetch_currency_data(source_currency=\"CHF\", target_currencies=\"EUR\"):\n",
    "    \"\"\"Fetch live exchange rates with customizable source and target currencies\"\"\"\n",
    "\n",
    "    # Ensure API key is available\n",
    "    if not API_KEY:\n",
    "        return {\"error\": \"API key not found\"}\n",
    "\n",
    "    # Build API request URL\n",
    "    base_url = \"https://api.apilayer.com/currency_data/\"\n",
    "    endpoint = \"live\"\n",
    "    api_url = f\"{base_url}{endpoint}?source={source_currency}&currencies={target_currencies}\"\n",
    "    headers = {\"apikey\": API_KEY}\n",
    "\n",
    "    # Send request to currency API & return parsed JSON if successful, otherwise return error message\n",
    "    response = requests.get(api_url, headers=headers)\n",
    "    return response.json() if response.status_code == 200 else {\"error\": response.text}\n",
    "\n",
    "def store_currency_data_in_rds(currency_data):\n",
    "    \"\"\"Store live exchange rates in an AWS RDS PostgreSQL database.\"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to DB...\")\n",
    "\n",
    "        # Establish DB connection using psycopg2\n",
    "        conn = psycopg2.connect(\n",
    "            host=ENDPOINT,\n",
    "            dbname=DB_NAME,\n",
    "            user=USERNAME,\n",
    "            password=PASSWORD\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        conn.set_session(autocommit=True)\n",
    "        print(\"DB connection successful.\")\n",
    "\n",
    "        # Define the table schema and columns\n",
    "        table_name = \"tbl_currency_data\"\n",
    "        columns = {\n",
    "            \"timestamp\": \"TIMESTAMP NOT NULL\",\n",
    "            \"source_currency\": \"VARCHAR(10)\",\n",
    "            \"target_currency\": \"VARCHAR(10)\",\n",
    "            \"exchange_rate\": \"FLOAT\"\n",
    "        }\n",
    "\n",
    "        # Create table dynamically if it doesn't exist\n",
    "        create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "        create_table_query += \", \".join([f\"{col} {datatype}\" for col, datatype in columns.items()])\n",
    "        create_table_query += \", PRIMARY KEY (timestamp, source_currency, target_currency));\"\n",
    "        cur.execute(create_table_query)\n",
    "\n",
    "        # Prepare INSERT statement with conflict handling\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} ({', '.join(columns.keys())})\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (timestamp, source_currency, target_currency) DO NOTHING;\n",
    "        \"\"\"\n",
    "\n",
    "        # Get current timestamp in milliseconds (trimmed to 3 decimals)\n",
    "        now = datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  \n",
    "\n",
    "        # Parse currency pairs and insert them into the DB\n",
    "        for pair, rate in currency_data.get(\"quotes\", {}).items():\n",
    "            source_currency, target_currency = pair[:3], pair[3:]\n",
    "            cur.execute(\n",
    "                insert_query,\n",
    "                (now, source_currency, target_currency, rate)\n",
    "            )\n",
    "\n",
    "        # Clean up DB connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "\n",
    "def convert_unix_to_iso(timestamp):\n",
    "    \"\"\"Convert Unix timestamp to ISO 8601 format\"\"\"\n",
    "    return datetime.utcfromtimestamp(timestamp).isoformat()\n",
    "\n",
    "def preprocess_currency_data(data):\n",
    "    \"\"\"Fix timestamps in the currency data JSON\"\"\"\n",
    "    if \"timestamp\" in data:\n",
    "        data[\"timestamp\"] = convert_unix_to_iso(data[\"timestamp\"])\n",
    "    return data\n",
    "\n",
    "def store_json_to_s3(currency_data):\n",
    "    \"\"\"Store the JSON data to S3 bucket in a structured way\"\"\"\n",
    "    # Initialize S3 client\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Generate file key based on today's date\n",
    "    today = str(datetime.today().date())\n",
    "    foldername = \"currency_data\"\n",
    "    key = f\"{foldername}/{foldername}_{today}.json\"\n",
    "\n",
    "    # Convert timestamps to ISO format\n",
    "    processed_data = preprocess_currency_data(currency_data)\n",
    "\n",
    "    # Upload JSON to S3\n",
    "    s3.put_object(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Key=key,\n",
    "        Body=json.dumps(processed_data, indent=2),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"AWS Lambda Entry Point\"\"\"\n",
    "    \n",
    "    # Step 1: Fetch exchange rate data from external API\n",
    "    currency_data = fetch_currency_data()\n",
    "    \n",
    "    # Step 2: Return early if fetch failed\n",
    "    if \"error\" in currency_data:\n",
    "        return {\n",
    "            \"statusCode\": 400,\n",
    "            \"body\": json.dumps({\"error\": currency_data[\"error\"]}, indent=2)\n",
    "        }\n",
    "    \n",
    "    # Step 3: Store data in PostgreSQL RDS\n",
    "    store_currency_data_in_rds(currency_data)\n",
    "\n",
    "    # Step 4: Store JSON data in S3\n",
    "    store_json_to_s3(currency_data)\n",
    "\n",
    "    # Step 5: Return success response\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps({\n",
    "            \"message\": \"Live exchange rates stored successfully in RDS and S3.\",\n",
    "            \"s3_path\": f\"s3://{S3_BUCKET}/currency_data/currency_rates_{datetime.today().strftime('%Y-%m-%d')}.json\",\n",
    "            \"currency_rates\": currency_data\n",
    "        }, indent=2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### group2_historical_data_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================================\n",
    "# AWS Lambda Function: Historical Currency Data Pipeline\n",
    "# Description: Fetches historical exchange rates from an external API and stores\n",
    "#              the data in an RDS PostgreSQL database and in an S3 bucket as JSON.\n",
    "# Technologies: AWS Lambda, S3, RDS (PostgreSQL), apilayer API\n",
    "# Author: Joshua HÃ¼gli\n",
    "# ===================================================================================\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import psycopg2\n",
    "import boto3\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Environment Configuration\n",
    "# Load sensitive configuration values from Lambda environment variables.\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "ENDPOINT = os.environ['ENDPOINT']       # RDS database host\n",
    "DB_NAME = os.environ['DB_NAME']         # Database name\n",
    "USERNAME = os.environ['USERNAME']       # Database username\n",
    "PASSWORD = os.environ['PASSWORD']       # Database password\n",
    "S3_BUCKET = os.environ['S3_BUCKET']     # S3 bucket name for JSON export\n",
    "API_KEY = os.environ['API_KEY']         # API key for currency data provider\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Fetch Historical Currency Data\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def fetch_historical_currency_data(start_date=\"2024-01-01\", end_date=\"2025-04-29\", source_currency=\"CHF\", target_currencies=\"EUR\"):\n",
    "    \"\"\"\n",
    "    Fetch historical exchange rate data from the apilayer API.\n",
    "\n",
    "    Args:\n",
    "        start_date (str): Start date in YYYY-MM-DD format.\n",
    "        end_date (str): End date in YYYY-MM-DD format.\n",
    "        source_currency (str): The base currency (default: CHF).\n",
    "        target_currencies (str): Comma-separated target currencies (default: EUR).\n",
    "\n",
    "    Returns:\n",
    "        dict: JSON response with historical exchange rates or error message.\n",
    "    \"\"\"\n",
    "    if not API_KEY:\n",
    "        return {\"error\": \"API key not found\"}\n",
    "\n",
    "    base_url = \"https://api.apilayer.com/currency_data/timeframe\"\n",
    "    params = f\"?start_date={start_date}&end_date={end_date}&source={source_currency}&currencies={target_currencies}\"\n",
    "    url = base_url + params\n",
    "    headers = {\"apikey\": API_KEY}\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json() if response.status_code == 200 else {\"error\": response.text}\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Store Historical Currency Data in RDS\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def store_historical_currency_data_in_rds(currency_data):\n",
    "    \"\"\"\n",
    "    Store historical exchange rates in a PostgreSQL RDS instance.\n",
    "\n",
    "    Args:\n",
    "        currency_data (dict): JSON object with exchange rates from API.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to DB...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=ENDPOINT,\n",
    "            dbname=DB_NAME,\n",
    "            user=USERNAME,\n",
    "            password=PASSWORD\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        conn.set_session(autocommit=True)\n",
    "        print(\"DB connection successful.\")\n",
    "\n",
    "        # Define schema\n",
    "        table_name = \"tbl_currency_data\"\n",
    "        columns = {\n",
    "            \"timestamp\": \"TIMESTAMP NOT NULL\",\n",
    "            \"source_currency\": \"VARCHAR(10)\",\n",
    "            \"target_currency\": \"VARCHAR(10)\",\n",
    "            \"exchange_rate\": \"FLOAT\"\n",
    "        }\n",
    "\n",
    "        # Create table if not exists\n",
    "        create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} (\"\n",
    "        create_table_query += \", \".join([f\"{col} {datatype}\" for col, datatype in columns.items()])\n",
    "        create_table_query += \", PRIMARY KEY (timestamp, source_currency, target_currency));\"\n",
    "        cur.execute(create_table_query)\n",
    "\n",
    "        # Prepare INSERT statement\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {table_name} ({', '.join(columns.keys())})\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (timestamp, source_currency, target_currency) DO NOTHING;\n",
    "        \"\"\"\n",
    "\n",
    "        # Insert all exchange rate records\n",
    "        for date_str, rates in currency_data.get(\"quotes\", {}).items():\n",
    "            for pair, rate in rates.items():\n",
    "                source_currency, target_currency = pair[:3], pair[3:]\n",
    "                timestamp = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "                cur.execute(insert_query, (timestamp, source_currency, target_currency, rate))\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Database error:\", e)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Store JSON Data in S3\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def store_json_to_s3(currency_data, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Store the fetched historical currency data as a JSON file in an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        currency_data (dict): JSON data from the API.\n",
    "        start_date (str): Start date of the data range.\n",
    "        end_date (str): End date of the data range.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    foldername = \"currency_data\"\n",
    "    key = f\"{foldername}/currency_data_{start_date}_to_{end_date}.json\"\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=S3_BUCKET,\n",
    "        Key=key,\n",
    "        Body=json.dumps(currency_data, indent=2),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# AWS Lambda Entry Point\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda function entry point for fetching and storing historical currency data.\n",
    "\n",
    "    Args:\n",
    "        event (dict): Event payload with optional 'start_date' and 'end_date'.\n",
    "        context (LambdaContext): Runtime information.\n",
    "\n",
    "    Returns:\n",
    "        dict: HTTP-style status and message with S3 path reference.\n",
    "    \"\"\"\n",
    "    # Default values if not provided in event\n",
    "    start_date = event.get(\"start_date\", \"2025-05-22\")\n",
    "    end_date = event.get(\"end_date\", \"2025-05-23\")\n",
    "\n",
    "    currency_data = fetch_historical_currency_data(start_date=start_date, end_date=end_date)\n",
    "\n",
    "    if \"error\" in currency_data:\n",
    "        return {\n",
    "            \"statusCode\": 400,\n",
    "            \"body\": json.dumps({\"error\": currency_data[\"error\"]}, indent=2)\n",
    "        }\n",
    "\n",
    "    store_historical_currency_data_in_rds(currency_data)\n",
    "    store_json_to_s3(currency_data, start_date, end_date)\n",
    "\n",
    "    return {\n",
    "        \"statusCode\": 200,\n",
    "        \"body\": json.dumps({\n",
    "            \"message\": \"Historical exchange rates stored successfully in RDS and S3.\",\n",
    "            \"s3_path\": f\"s3://{S3_BUCKET}/currency_data/currency_data_{start_date}_to_{end_date}.json\",\n",
    "            \"currency_rates\": currency_data\n",
    "        }, indent=2)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzKIpRVe8cYj"
   },
   "source": [
    "## Lamdba Functions for Data Processing in Datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpJOe3Hf8qNP"
   },
   "source": [
    "### group2_dwh_fact_energy_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ichuJmA8a_Q"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Load environment variables for both databases\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"var_host\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password\")\n",
    "}\n",
    "DB_CONFIG2 = {\n",
    "    \"host\": os.getenv(\"var_host2\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database2\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password2\")\n",
    "}\n",
    "def lambda_handler(event, context):\n",
    "\n",
    "    try:\n",
    "        # Establish database connections\n",
    "        conn1 = psycopg2.connect(**DB_CONFIG)\n",
    "        conn2 = psycopg2.connect(**DB_CONFIG2)\n",
    "        cur2 = conn2.cursor()\n",
    "\n",
    "        # Define the date for which to fetch data (2 days before today)\n",
    "        yesterday = date.today() - timedelta(days=2)\n",
    "        start_date = date.today() - timedelta(days=2)\n",
    "\n",
    "        # Query data from the data lake\n",
    "        query_energy = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM tbl_energy_production_data\n",
    "            WHERE timestamp >= '{start_date}'::date\n",
    "            AND timestamp < '{yesterday + timedelta(days=1)}'::date\n",
    "        \"\"\"\n",
    "        df_energy = pd.read_sql_query(query_energy, conn1)\n",
    "\n",
    "        # Load dimension tables from the data warehouse\n",
    "        df_locations = pd.read_sql_query(\"SELECT * FROM dim_locations\", conn2)\n",
    "        df_countries = pd.read_sql_query(\"SELECT * FROM dim_countries\", conn2)\n",
    "        df_time = pd.read_sql_query(\"SELECT * FROM dim_time\", conn2)\n",
    "\n",
    "        # Merge with time dimension to get time_id\n",
    "        df_merged = df_energy.merge(\n",
    "            df_time[['time_id', 'timestamp_utc']],\n",
    "            left_on='timestamp',\n",
    "            right_on='timestamp_utc',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Select and rename relevant columns\n",
    "        df_fact = df_merged[[\n",
    "            'time_id', 'timestamp', 'nuclear', 'solar', 'wind_onshore', 'load',\n",
    "            'residual_load', 'hydro_run_of_river',\n",
    "            'hydro_water_reservoir', 'hydro_pumped_storage'\n",
    "        ]].copy()\n",
    "        df_fact.rename(columns={\n",
    "            'nuclear': 'nuclear_output',\n",
    "            'solar': 'solar_output',\n",
    "            'wind_onshore': 'wind_output',\n",
    "            'hydro_run_of_river': 'hydro_run_of_river_output',\n",
    "            'hydro_water_reservoir': 'hydro_water_reservoir_output',\n",
    "            'hydro_pumped_storage': 'hydro_pumped_storage_output'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Add country_id for Switzerland\n",
    "        switzerland_id = df_countries.loc[\n",
    "            df_countries['iso_code'] == 'CH', 'country_id'\n",
    "        ].values[0]\n",
    "        df_fact.insert(\n",
    "            loc=df_fact.columns.get_loc('time_id') + 1,\n",
    "            column='country_id',\n",
    "            value=switzerland_id\n",
    "        )\n",
    "\n",
    "        # Fetch existing time_ids from the target table to avoid duplicates\n",
    "        cur2.execute(\"SELECT time_id FROM fact_energy_production\")\n",
    "        existing_time_ids = {row[0] for row in cur2.fetchall()}\n",
    "\n",
    "        # Keep only records with time_ids not already in the table\n",
    "        df_fact = df_fact[~df_fact['time_id'].isin(existing_time_ids)]\n",
    "\n",
    "        # If no new data to insert, exit early\n",
    "        if df_fact.empty:\n",
    "            cur2.close()\n",
    "            conn1.close()\n",
    "            conn2.close()\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps('No new records to insert.')\n",
    "            }\n",
    "\n",
    "        # Get the current maximum production_id to generate unique keys\n",
    "        cur2.execute(\"SELECT COALESCE(MAX(production_id), 0) FROM fact_energy_production\")\n",
    "        max_production_id = cur2.fetchone()[0]\n",
    "\n",
    "        # Add production_id as a running index starting from the max + 1\n",
    "        df_fact.insert(0, 'production_id', range(max_production_id + 1, max_production_id + 1 + len(df_fact)))\n",
    "\n",
    "        # Prepare columns and values for insertion\n",
    "        columns = [\n",
    "            \"production_id\", \"time_id\", \"country_id\", \"timestamp\", \"solar_output\",\n",
    "            \"wind_output\", \"load\", \"residual_load\", 'hydro_run_of_river_output',\n",
    "            'hydro_water_reservoir_output', 'hydro_pumped_storage_output', 'nuclear_output'\n",
    "        ]\n",
    "        values = [tuple(x) for x in df_fact[columns].to_numpy()]\n",
    "\n",
    "        # Create SQL INSERT statement\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO fact_energy_production ({', '.join(columns)})\n",
    "        VALUES %s\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute batch insert using execute_values for efficiency\n",
    "        execute_values(cur2, insert_sql, values)\n",
    "\n",
    "        # Commit transaction and close connections\n",
    "        conn2.commit()\n",
    "        cur2.close()\n",
    "        conn1.close()\n",
    "        conn2.close()\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('New records successfully inserted.')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps(f'Error during processing: {str(e)}')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pI5PRsW9IJM"
   },
   "source": [
    "### group2_dwh_fact_energy_trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcPeBwUf9JBK"
   },
   "outputs": [],
   "source": [
    "# STEP 1: Connect to Data Sources\n",
    "def connect_to_db(endpoint: str, db_name: str, username: str, password: str) -> psycopg2.extensions.connection:\n",
    "    \"\"\"\n",
    "    Establish a connection to a PostgreSQL database.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=endpoint,\n",
    "        dbname=db_name,\n",
    "        user=username,\n",
    "        password=password\n",
    "    )\n",
    "    conn.set_session(autocommit=True)\n",
    "    return conn\n",
    "\n",
    "# STEP 2: Utility for Extracting Data from DB\n",
    "def get_data_from_db(\n",
    "    conn: psycopg2.extensions.connection,\n",
    "    table_name: str,\n",
    "    expression: str = '*',\n",
    "    condition: str = ''\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query data from a database table and return as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT {expression} FROM {table_name}\"\n",
    "    if condition != '':\n",
    "        query += f\" WHERE {condition}\"\n",
    "    query += ';'\n",
    "    print(\"Query Performed: \" + query)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "# STEP 4: Calculate Sunshine Percentages by Hour\n",
    "def calculate_sunshine_percentage(df_sun: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of sunshine for each hour of the day.\n",
    "    \"\"\"\n",
    "\n",
    "    def time_to_float(t) -> float:\n",
    "        if isinstance(t, str):\n",
    "            h, m, s = map(int, t.split(':'))\n",
    "        else:\n",
    "            h, m, s = t.hour, t.minute, t.second\n",
    "        return h + m/60 + s/3600\n",
    "\n",
    "    df = df_sun.copy()\n",
    "    df['sunrise_float'] = df['sunrise'].apply(time_to_float)\n",
    "    df['sunset_float'] = df['sunset'].apply(time_to_float)\n",
    "\n",
    "    for hour in range(24):\n",
    "        def hour_percentage(row) -> float:\n",
    "            start = row['sunrise_float']\n",
    "            end = row['sunset_float']\n",
    "            hour_start = hour\n",
    "            hour_end = hour + 1\n",
    "            if end <= hour_start or start >= hour_end:\n",
    "                return 0.0\n",
    "            overlap_start = max(start, hour_start)\n",
    "            overlap_end = min(end, hour_end)\n",
    "            return max(0.0, (overlap_end - overlap_start)) * 100\n",
    "\n",
    "        df[f'sunshine_pct_hour_{hour}'] = df.apply(hour_percentage, axis=1)\n",
    "\n",
    "    # Reshape into long format with datetime\n",
    "    df = df.melt(\n",
    "        id_vars=['city', 'date'],\n",
    "        value_vars=[f'sunshine_pct_hour_{hour}' for hour in range(24)],\n",
    "        var_name='hour',\n",
    "        value_name='sunshine_pct_hour'\n",
    "    )\n",
    "    df['hour'] = df['hour'].str.extract(r'(\\d+)').astype(int)\n",
    "    df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hour'], unit='h')\n",
    "    df = df.drop(['date', 'hour'], axis=1)\n",
    "    return df\n",
    "\n",
    "# STEP 9: Upload Final Data to Warehouse\n",
    "def upload_to_db(conn: psycopg2.extensions.connection, table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Upload a DataFrame to a database table.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                weather_id       INTEGER PRIMARY KEY,\n",
    "                time_id          INTEGER REFERENCES dim_time(time_id),\n",
    "                location_id      INTEGER REFERENCES dim_location(location_id),\n",
    "                wind_speed       FLOAT,\n",
    "                sunshine_minutes FLOAT\n",
    "            );\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "    insert_query = f'''\n",
    "        INSERT INTO {table_name} (weather_id, time_id, location_id, wind_speed, sunshine_minutes)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    '''\n",
    "    data_tuples = df.where(pd.notnull(df), None).values.tolist()\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        for row in data_tuples:\n",
    "            cur.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created and data uploaded successfully.\")\n",
    "\n",
    "# Main ETL Handler\n",
    "def lambda_handler():\n",
    "    \"\"\"\n",
    "    Main ETL handler function to process and upload weather data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # STEP 1: Connect to Data Lake and Warehouse\n",
    "        print(\"Connecting to Datalake \")\n",
    "        dl_conn = connect_to_db(dl_endpoint, dl_db_name, username, dl_password)\n",
    "        print(\"Connection to Datalake successful.\")\n",
    "\n",
    "        print(\"Connecting to WareHouse \")\n",
    "        wh_conn = connect_to_db(wh_endpoint, wh_db_name, username, wh_password)\n",
    "        print(\"Connection to WareHouse successful.\")\n",
    "\n",
    "        # STEP 2: Identify Last Stored Data in Warehouse\n",
    "        df_init_fact = get_data_from_db(wh_conn, 'fact_weather', 'max(time_id),max(weather_id)')\n",
    "        df = get_data_from_db(wh_conn, 'dim_time', 'timestamp_utc', f'time_id = {df_init_fact.iloc[0,0]}')\n",
    "        print(\"Last timestamp in warehouse: \", df.iloc[0,0])\n",
    "\n",
    "        # STEP 3: Extract New Data from Data Lake\n",
    "        df_sun = get_data_from_db(conn=dl_conn, table_name='tbl_weather_sun_data', condition= f\"date >= '{df.iloc[0,0].date()}'\")\n",
    "        df_sun = calculate_sunshine_percentage(df_sun)\n",
    "        print(\"Sunshine data downloaded.\")\n",
    "\n",
    "        df_weather = get_data_from_db(conn=dl_conn, table_name='tbl_weather_data', condition= f\"datetime > '{df.iloc[0,0]}'\")\n",
    "        print(\"Weather data downloaded.\")\n",
    "\n",
    "        if df_weather.empty:\n",
    "            print(\"No new weather data available.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"New weather data available.\")\n",
    "\n",
    "        # STEP 5: Merge Weather and Sunshine\n",
    "        df_ws = df_weather.merge(df_sun, on=['city', 'datetime'], how='inner')\n",
    "        df_ws['sunshine_minutes'] = (100 - df_ws['clouds_all']) * df_ws['sunshine_pct_hour'] / 10000 * 60\n",
    "\n",
    "        # STEP 6: Clean and Filter Data\n",
    "        df_ws = df_ws.drop(['id','pressure','humidity','temp_min','temp_max','temp','weather_main','weather_description','rain_1h','clouds_all','sunshine_pct_hour'], axis=1)\n",
    "        print(\"Data successfully processed.\")\n",
    "\n",
    "        # STEP 7: Enrich with Location Info\n",
    "        df_city = get_data_from_db(wh_conn, 'dim_locations')\n",
    "        df_city_s = df_city[['location_id', 'city']]\n",
    "        df_joined_wl = df_ws.merge(df_city_s, on=['city'], how='outer')\n",
    "        df_joined_wl = df_joined_wl.drop(['city'], axis=1)\n",
    "\n",
    "        # STEP 7: Enrich with Time Info\n",
    "        df_time = get_data_from_db(wh_conn, 'dim_time')\n",
    "        df_time_s = df_time[['time_id', 'timestamp_utc']]\n",
    "        df_time_s.columns = ['time_id', 'datetime']\n",
    "        df_joined_wld = df_joined_wl.merge(df_time_s, on=['datetime'], how='inner')\n",
    "        df_joined_wld = df_joined_wld.drop(['datetime'], axis=1)\n",
    "\n",
    "        # STEP 8: Assign Identifiers\n",
    "        df_joined_wld['weather_id'] = df_joined_wld.index + df_init_fact.iloc[0,1] + 1\n",
    "\n",
    "        # STEP 9: Load to Warehouse\n",
    "        upload_to_db(wh_conn, 'fact_weather', df_joined_wld[['weather_id', 'time_id', 'location_id', 'wind_speed', 'sunshine_minutes']])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cUn8Wae9VnH"
   },
   "source": [
    "### group2_dwh_fact_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwfIZwRd9XNO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def connect_to_db(endpoint: str, db_name: str, username: str, password: str) -> psycopg2.extensions.connection:\n",
    "    \"\"\"\n",
    "    Establish a connection to a PostgreSQL database.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=endpoint,\n",
    "        dbname=db_name,\n",
    "        user=username,\n",
    "        password=password\n",
    "    )\n",
    "    conn.set_session(autocommit=True)\n",
    "    return conn\n",
    "\n",
    "def get_data_from_db(\n",
    "    conn: psycopg2.extensions.connection,\n",
    "    table_name: str,\n",
    "    expression: str = '*',\n",
    "    condition: str = ''\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query data from a database table and return as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT {expression} FROM {table_name}\"\n",
    "    if condition != '':\n",
    "        query += f\" WHERE {condition}\"\n",
    "    query += ';'\n",
    "    print(\"Query Performed: \" + query)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "def calculate_sunshine_percentage(df_sun: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of sunshine for each hour of the day.\n",
    "    \"\"\"\n",
    "\n",
    "    def time_to_float(t) -> float:\n",
    "        # Convert time string or datetime.time to float hours\n",
    "        if isinstance(t, str):\n",
    "            h, m, s = map(int, t.split(':'))\n",
    "        else:\n",
    "            h, m, s = t.hour, t.minute, t.second\n",
    "        return h + m/60 + s/3600\n",
    "\n",
    "    df = df_sun.copy()\n",
    "    df['sunrise_float'] = df['sunrise'].apply(time_to_float)\n",
    "    df['sunset_float'] = df['sunset'].apply(time_to_float)\n",
    "\n",
    "    for hour in range(24):\n",
    "        def hour_percentage(row) -> float:\n",
    "            start = row['sunrise_float']\n",
    "            end = row['sunset_float']\n",
    "            hour_start = hour\n",
    "            hour_end = hour + 1\n",
    "            # No sun during this hour\n",
    "            if end <= hour_start or start >= hour_end:\n",
    "                return 0.0\n",
    "            # Sun is up for part or all of this hour\n",
    "            overlap_start = max(start, hour_start)\n",
    "            overlap_end = min(end, hour_end)\n",
    "            return max(0.0, (overlap_end - overlap_start)) * 100\n",
    "\n",
    "        df[f'sunshine_pct_hour_{hour}'] = df.apply(hour_percentage, axis=1)\n",
    "\n",
    "    # Melt the DataFrame to have columns: city, date, hour, sunshine_pct_hour\n",
    "    df = df.melt(\n",
    "        id_vars=['city', 'date'],\n",
    "        value_vars=[f'sunshine_pct_hour_{hour}' for hour in range(24)],\n",
    "        var_name='hour',\n",
    "        value_name='sunshine_pct_hour'\n",
    "    )\n",
    "    # Extract the hour as integer from the column name\n",
    "    df['hour'] = df['hour'].str.extract(r'(\\d+)').astype(int)\n",
    "    df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hour'], unit='h')\n",
    "    df = df.drop(['date', 'hour'], axis=1)\n",
    "    return df\n",
    "\n",
    "def upload_to_db(conn: psycopg2.extensions.connection, table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Upload a DataFrame to a database table.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # Create table if it does not exist\n",
    "        cur.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                weather_id       INTEGER PRIMARY KEY,\n",
    "                time_id          INTEGER REFERENCES dim_time(time_id),\n",
    "                location_id      INTEGER REFERENCES dim_location(location_id),\n",
    "                wind_speed       FLOAT,\n",
    "                sunshine_minutes FLOAT\n",
    "            );\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "    # Insert data row by row\n",
    "    insert_query = f'''\n",
    "        INSERT INTO {table_name} (weather_id, time_id, location_id, wind_speed, sunshine_minutes)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    '''\n",
    "    data_tuples = df.where(pd.notnull(df), None).values.tolist()\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        for row in data_tuples:\n",
    "            cur.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created and data uploaded successfully.\")\n",
    "\n",
    "def lambda_handler():\n",
    "    \"\"\"\n",
    "    Main ETL handler function to process and upload weather data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to Datalake \")\n",
    "        dl_conn = connect_to_db(dl_endpoint, dl_db_name, username, dl_password)\n",
    "        print(\"Connection to Datalake successful.\")\n",
    "\n",
    "        # Connect to warehouse\n",
    "        print(\"Connecting to WareHouse \")\n",
    "        wh_conn = connect_to_db(wh_endpoint, wh_db_name, username, wh_password)\n",
    "        print(\"Connection to WareHouse successful.\")\n",
    "\n",
    "        # Get the current situation in the warehouse\n",
    "        df_init_fact = get_data_from_db(wh_conn, 'fact_weather', 'max(time_id),max(weather_id)')\n",
    "        df = get_data_from_db(wh_conn, 'dim_time', 'timestamp_utc', f'time_id = {df_init_fact.iloc[0,0]}')\n",
    "        print(\"Last timestamp in warehouse: \", df.iloc[0,0])\n",
    "\n",
    "        # Download sunshine data from datalake\n",
    "        df_sun = get_data_from_db(conn=dl_conn, table_name='tbl_weather_sun_data', condition= f\"date >= '{df.iloc[0,0].date()}'\")\n",
    "        df_sun = calculate_sunshine_percentage(df_sun)\n",
    "        print(\"Sunshine data downloaded.\")\n",
    "\n",
    "        # Download weather data from datalake\n",
    "        df_weather = get_data_from_db(conn=dl_conn, table_name='tbl_weather_data', condition= f\"datetime > '{df.iloc[0,0]}'\")\n",
    "        print(\"Weather data downloaded.\")\n",
    "\n",
    "        if df_weather.empty:\n",
    "            print(\"No new weather data available.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"New weather data available.\")\n",
    "\n",
    "        # Calculate the sunshine duration in minutes\n",
    "        df_ws = df_weather.merge(df_sun, on=['city', 'datetime'], how='inner')\n",
    "        df_ws['sunshine_minutes'] = (100 - df_ws['clouds_all']) * df_ws['sunshine_pct_hour'] / 10000 * 60\n",
    "\n",
    "        # Clean up the DataFrame\n",
    "        df_ws = df_ws.drop(['id','pressure','humidity','temp_min','temp_max','temp','weather_main','weather_description','rain_1h','clouds_all','sunshine_pct_hour'], axis=1)\n",
    "        print(\"Data successfully processed.\")\n",
    "\n",
    "        # Join with city data\n",
    "        df_city = get_data_from_db(wh_conn, 'dim_locations')\n",
    "        df_city_s = df_city[['location_id', 'city']]\n",
    "        df_joined_wl = df_ws.merge(df_city_s, on=['city'], how='outer')\n",
    "        df_joined_wl = df_joined_wl.drop(['city'], axis=1)\n",
    "\n",
    "        # Join with time data\n",
    "        df_time = get_data_from_db(wh_conn, 'dim_time')\n",
    "        df_time_s = df_time[['time_id', 'timestamp_utc']]\n",
    "        df_time_s.columns = ['time_id', 'datetime']\n",
    "        df_joined_wld = df_joined_wl.merge(df_time_s, on=['datetime'], how='inner')\n",
    "        df_joined_wld = df_joined_wld.drop(['datetime'], axis=1)\n",
    "        df_joined_wld['weather_id'] = df_joined_wld.index + df_init_fact.iloc[0,1] + 1\n",
    "\n",
    "        # Upload data to database\n",
    "        upload_to_db(wh_conn, 'fact_weather', df_joined_wld[['weather_id', 'time_id', 'location_id', 'wind_speed', 'sunshine_minutes']])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "    return df_joined_wld\n",
    "\n",
    "df = lambda_handler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6Xgxh-B9YJj"
   },
   "source": [
    "## SQL Queries for DataLake and Datawarehoue tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8-heXg3-fAk"
   },
   "source": [
    "### DataLake tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0LV1AfLAcRH"
   },
   "source": [
    "tbl_currency_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRRbxVoAAfVi"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_currency_data (\n",
    "\t\"timestamp\" timestamp NOT NULL,\n",
    "\tsource_currency varchar(10) NOT NULL,\n",
    "\ttarget_currency varchar(10) NOT NULL,\n",
    "\texchange_rate float8 NULL,\n",
    "\tCONSTRAINT tbl_currency_data_pkey PRIMARY KEY (\"timestamp\", source_currency, target_currency)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbpHaDJrAjec"
   },
   "source": [
    "tbl_energy_cbet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUuXq7zOArkJ"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_cbet_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tcountry text NULL,\n",
    "\tvalue float8 NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP6u8KJVA-Op"
   },
   "source": [
    "tbl_energy_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfR7xI6VAyLq"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_price_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tprice float8 NULL,\n",
    "\tunit text NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbKTD0oAtZp"
   },
   "source": [
    "tbl_energy_production_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzIwVwGMA2CH"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_production_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tcross_border_electricity_trading float8 NULL,\n",
    "\tnuclear float8 NULL,\n",
    "\thydro_run_of_river float8 NULL,\n",
    "\thydro_water_reservoir float8 NULL,\n",
    "\thydro_pumped_storage float8 NULL,\n",
    "\t\"others\" float8 NULL,\n",
    "\twind_onshore float8 NULL,\n",
    "\tsolar float8 NULL,\n",
    "\t\"load\" float8 NULL,\n",
    "\tresidual_load float8 NULL,\n",
    "\trenewable_share_of_load float8 NULL,\n",
    "\trenewable_share_of_generation float8 NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vyRQE-sAZLa"
   },
   "source": [
    "tbl_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8toZzvyf-tFw"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_weather_data (\n",
    "\tid serial4 NOT NULL,\n",
    "\tcity varchar(100) NULL,\n",
    "\tdatetime timestamp NULL,\n",
    "\t\"temp\" float8 NULL,\n",
    "\tpressure int4 NULL,\n",
    "\thumidity int4 NULL,\n",
    "\ttemp_min float8 NULL,\n",
    "\ttemp_max float8 NULL,\n",
    "\twind_speed float8 NULL,\n",
    "\tweather_main varchar(50) NULL,\n",
    "\tweather_description text NULL,\n",
    "\train_1h float8 NULL,\n",
    "\tclouds_all float8 NULL,\n",
    "\tCONSTRAINT tbl_weather_data_pkey PRIMARY KEY (id)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFPH_q7R-tdo"
   },
   "source": [
    "### Datawarehouse tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKeI_nTq-3Zi"
   },
   "source": [
    "dim_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8qBmVwX-4Wf",
    "outputId": "e02c8667-3bc6-4e99-d8b0-218884d3f054"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4177214843.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mCREATE TABLE dim_countries (\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CREATE TABLE dim_countries (\n",
    "    country_id SERIAL PRIMARY KEY,\n",
    "    country_name_en VARCHAR(100),\n",
    "    country_name_de VARCHAR(100),\n",
    "    iso_code VARCHAR(5)\n",
    ");\n",
    "\n",
    "INSERT INTO dim_countries (country_name_en, country_name_de, iso_code)\n",
    "VALUES\n",
    "    ('Germany',      'Deutschland',   'DE'),\n",
    "    ('France',       'Frankreich',    'FR'),\n",
    "    ('Austria',      'Ã–sterreich',    'AT'),\n",
    "    ('Italy',        'Italien',       'IT'),\n",
    "    ('Liechtenstein','Liechtenstein', 'LI'),\n",
    "    ('Switzerland',    'Schweiz',     'CH');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2y75raLBVlu"
   },
   "source": [
    "dim_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5wApLGOBR9S"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS public.dim_currency (\n",
    "\tcurrency_id serial4 NOT NULL,\n",
    "\tcurrency_code varchar(3) NOT NULL,\n",
    "\tCONSTRAINT dim_currency_pkey PRIMARY KEY (currency_id)\n",
    ");\n",
    "\n",
    "INSERT INTO dim_currency (currency_id, currency_code) VALUES\n",
    "(1, 'CHF'),\n",
    "(2, 'EUR');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stsh12dz_A6V"
   },
   "source": [
    "dim_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMrQHLOw_Bnq"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE dim_locations (\n",
    "    location_id SERIAL PRIMARY KEY,\n",
    "    country VARCHAR(100) DEFAULT 'Switzerland',\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    long DOUBLE PRECISION NOT NULL\n",
    ");\n",
    "\n",
    "-- Daten einfÃ¼gen\n",
    "INSERT INTO dim_locations (city, lat, long)\n",
    "VALUES\n",
    "    ('Aarau', 47.392715, 8.044445),\n",
    "    ('Baden', 47.473683, 8.308682),\n",
    "    ('Basel', 47.558108, 7.587826),\n",
    "    ('Bern', 46.948474, 7.452175),\n",
    "    ('Chur', 46.854747, 9.526490),\n",
    "    ('Frauenfeld', 47.556191, 8.896335),\n",
    "    ('Genf', 46.201756, 6.146601),\n",
    "    ('Lausanne', 46.521827, 6.632702),\n",
    "    ('Lugano', 46.005010, 8.952028),\n",
    "    ('Luzern', 47.050545, 8.305468),\n",
    "    ('Neuenburg', 46.989583, 6.929264),\n",
    "    ('Schaffhausen', 47.696049, 8.634513),\n",
    "    ('Sion', 46.231175, 7.358879),\n",
    "    ('Solothurn', 47.208135, 7.538405),\n",
    "    ('St. Gallen', 47.425059, 9.376588),\n",
    "    ('Winterthur', 47.499172, 8.729150),\n",
    "    ('Zug', 47.167990, 8.517365),\n",
    "    ('ZÃ¼rich', 47.374449, 8.541042);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whz-8X1qBgFc"
   },
   "source": [
    "dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ6008UcBgx4"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.dim_time (\n",
    "\ttime_id serial4 NOT NULL,\n",
    "\ttimestamp_utc timestamp NOT NULL,\n",
    "\t\"date\" date NOT NULL,\n",
    "\t\"hour\" int4 NULL,\n",
    "\tday_of_week varchar(3) NULL,\n",
    "\t\"month\" varchar(3) NULL,\n",
    "\t\"year\" int4 NULL,\n",
    "\tCONSTRAINT dim_time_day_of_week_check CHECK (((day_of_week)::text = ANY ((ARRAY['Mon'::character varying, 'Tue'::character varying, 'Wed'::character varying, 'Thu'::character varying, 'Fri'::character varying, 'Sat'::character varying, 'Sun'::character varying])::text[]))),\n",
    "\tCONSTRAINT dim_time_hour_check CHECK (((hour >= 0) AND (hour <= 23))),\n",
    "\tCONSTRAINT dim_time_month_check CHECK (((month)::text = ANY ((ARRAY['Jan'::character varying, 'Feb'::character varying, 'Mar'::character varying, 'Apr'::character varying, 'May'::character varying, 'Jun'::character varying, 'Jul'::character varying, 'Aug'::character varying, 'Sep'::character varying, 'Oct'::character varying, 'Nov'::character varying, 'Dec'::character varying])::text[]))),\n",
    "\tCONSTRAINT dim_time_pkey PRIMARY KEY (time_id),\n",
    "\tCONSTRAINT dim_time_year_check CHECK ((year >= 1900))\n",
    ");\n",
    "\n",
    "WITH RECURSIVE time_gen AS (\n",
    "    SELECT \n",
    "        TIMESTAMP '2024-01-01 00:00:00' AS ts\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        ts + INTERVAL '1 hour'\n",
    "    FROM time_gen\n",
    "    WHERE ts + INTERVAL '1 hour' <= TIMESTAMP '2025-06-30 23:00:00'\n",
    ")\n",
    "INSERT INTO dim_time (\n",
    "    timestamp_utc,\n",
    "    date,\n",
    "    hour,\n",
    "    day_of_week,\n",
    "    month,\n",
    "    year\n",
    ")\n",
    "SELECT\n",
    "    ts AS timestamp_utc,\n",
    "    ts::date AS date,\n",
    "    EXTRACT(HOUR FROM ts)::int AS hour,\n",
    "    TO_CHAR(ts, 'Dy') AS day_of_week,\n",
    "    TO_CHAR(ts, 'Mon') AS month,\n",
    "    EXTRACT(YEAR FROM ts)::int AS year\n",
    "FROM time_gen;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX65dSxc9kcO"
   },
   "source": [
    "fact_energy_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv-Ah7s89ihb"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE fact_energy_production (\n",
    "    production_id SERIAL PRIMARY KEY,\n",
    "    time_id INTEGER NOT NULL,\n",
    "    country_id INTEGER NOT NULL,\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    solar_output FLOAT,\n",
    "    wind_output FLOAT,\n",
    "    load FLOAT,\n",
    "    residual_load FLOAT,\n",
    "    hydro_run_of_river_output FLOAT,\n",
    "    hydro_water_reservoir_output FLOAT,\n",
    "    hydro_pumped_storage_output FLOAT,\n",
    "    nuclear_output FLOAT\n",
    ");\n",
    "\n",
    "-- Foreign Key\n",
    "ALTER TABLE fact_energy_production\n",
    "ADD CONSTRAINT fk_time\n",
    "FOREIGN KEY (time_id) REFERENCES dim_time(time_id);\n",
    "\n",
    "ALTER TABLE fact_energy_production\n",
    "ADD CONSTRAINT fk_country\n",
    "FOREIGN KEY (country_id) REFERENCES dim_countries(country_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-B7T8u6Bd0J"
   },
   "source": [
    "fact_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UjUunDUbNxn"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE IF NOT EXISTS fact_weather (\n",
    "    weather_id       INTEGER PRIMARY KEY,\n",
    "    time_id          INTEGER REFERENCES dim_time(time_id),\n",
    "    location_id      INTEGER REFERENCES dim_location(location_id),\n",
    "    temperature      FLOAT,\n",
    "    wind_speed       FLOAT,\n",
    "    sunshine_minutes FLOAT\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create keys in fact_energy_trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER TABLE fact_energy_trade\n",
    "ADD CONSTRAINT fk_time_id\n",
    "FOREIGN KEY (time_id)\n",
    "REFERENCES dim_time(time_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKIoSIc9bX_m"
   },
   "source": [
    "View for persona 1 dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFhaCRSubNxn"
   },
   "outputs": [],
   "source": [
    "#View for Dashboard of persona 1 (production data)\n",
    "CREATE OR REPLACE VIEW vw_energy_production_long AS\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Solar Output' AS type,\n",
    "  solar_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Wind Output' AS type,\n",
    "  wind_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Hydro Run-of-River Output' AS type,\n",
    "  hydro_run_of_river_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Hydro Water Reservoir Output' AS type,\n",
    "  hydro_water_reservoir_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Hydro Pumped Storage Output' AS type,\n",
    "  hydro_pumped_storage_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT\n",
    "  production_id,\n",
    "  time_id,\n",
    "  \"timestamp\",\n",
    "  'Nuclear Output' AS type,\n",
    "  nuclear_output AS value,\n",
    "  COALESCE(solar_output, 0)\n",
    "    + COALESCE(wind_output, 0)\n",
    "    + COALESCE(hydro_run_of_river_output, 0)\n",
    "    + COALESCE(hydro_water_reservoir_output, 0)\n",
    "    + COALESCE(hydro_pumped_storage_output, 0)\n",
    "    + COALESCE(nuclear_output, 0) AS total_output\n",
    "FROM fact_energy_production;\n",
    "\n",
    "\n",
    "#View for Dashboard of persona 1 (trading data)\n",
    "CREATE OR REPLACE VIEW public.vw_energy_trade_net_per_country_hourly\n",
    "AS WITH base AS (\n",
    "         SELECT ft.trade_id,\n",
    "            ft.time_id,\n",
    "            dt.timestamp_utc,\n",
    "            ft.neighbor_country_id,\n",
    "            ft.direction,\n",
    "            ft.energy_value_gw,\n",
    "            ft.value_eur,\n",
    "            ft.value_chf,\n",
    "            ft.exchange_rate,\n",
    "            sum(\n",
    "                CASE\n",
    "                    WHEN ft.direction = 'export'::text THEN ft.energy_value_gw\n",
    "                    ELSE - ft.energy_value_gw\n",
    "                END) OVER (PARTITION BY ft.time_id) AS total_net_energy_gw_per_time\n",
    "           FROM fact_energy_trade ft\n",
    "             JOIN dim_time dt ON ft.time_id = dt.time_id\n",
    "        )\n",
    " SELECT time_id,\n",
    "    timestamp_utc,\n",
    "    neighbor_country_id,\n",
    "    sum(\n",
    "        CASE\n",
    "            WHEN direction = 'export'::text THEN energy_value_gw\n",
    "            ELSE - energy_value_gw\n",
    "        END) AS net_energy_gw,\n",
    "    sum(\n",
    "        CASE\n",
    "            WHEN direction = 'export'::text THEN value_eur\n",
    "            ELSE - value_eur\n",
    "        END) AS net_value_eur,\n",
    "    sum(\n",
    "        CASE\n",
    "            WHEN direction = 'export'::text THEN value_chf\n",
    "            ELSE - value_chf\n",
    "        END) AS net_value_chf,\n",
    "    max(exchange_rate) AS exchange_rate,\n",
    "    max(total_net_energy_gw_per_time) AS total_net_energy_gw_per_time,\n",
    "        CASE\n",
    "            WHEN sum(\n",
    "            CASE\n",
    "                WHEN direction = 'export'::text THEN value_eur\n",
    "                ELSE - value_eur\n",
    "            END) = 0::double precision THEN NULL::double precision\n",
    "            ELSE (sum(\n",
    "            CASE\n",
    "                WHEN direction = 'export'::text THEN value_chf\n",
    "                ELSE - value_chf\n",
    "            END) / sum(\n",
    "            CASE\n",
    "                WHEN direction = 'export'::text THEN value_eur\n",
    "                ELSE - value_eur\n",
    "            END) - 1::double precision) * 100::double precision\n",
    "        END AS fx_difference_percent\n",
    "   FROM base\n",
    "  GROUP BY time_id, timestamp_utc, neighbor_country_id\n",
    "  ORDER BY timestamp_utc, neighbor_country_id;"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
