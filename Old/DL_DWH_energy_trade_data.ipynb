{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ec42c",
   "metadata": {},
   "source": [
    "# DWH fact_energy_trade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c22f9",
   "metadata": {},
   "source": [
    "## Energy trade table creation\n",
    "\n",
    "Lambda layer: group2_dwh_fact_energy_trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Configuration: Load environment variables for database access\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "DB_CONFIG_DATALAKE = {\n",
    "    \"host\": os.getenv(\"var_host\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password\")\n",
    "}\n",
    "\n",
    "DB_CONFIG_DWH = {\n",
    "    \"host\": os.getenv(\"var_host2\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database2\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password2\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    AWS Lambda function to extract, transform, and load hourly energy trade data\n",
    "    from a Datalake to a DWH (fact_energy_trade). It processes data for one day (2 days ago),\n",
    "    enriches it with currency rates, energy prices, and dimension lookups, and loads\n",
    "    it into the fact table.\n",
    "\n",
    "    Args:\n",
    "        event (dict): Lambda event payload.\n",
    "        context (LambdaContext): Lambda context object.\n",
    "\n",
    "    Returns:\n",
    "        dict: Status message and number of inserted rows or error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for days_ago in range(2, 3):  # Loop allows extension to more days if needed\n",
    "            trade_day = date.today() - timedelta(days=days_ago)\n",
    "            start_time = datetime.combine(trade_day, datetime.min.time())\n",
    "            end_time = start_time + timedelta(days=1)\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Connect to source (Datalake) and target (DWH) databases\n",
    "            # ------------------------------------------------------------------\n",
    "            conn_src = psycopg2.connect(**DB_CONFIG_DATALAKE)\n",
    "            conn_tgt = psycopg2.connect(**DB_CONFIG_DWH)\n",
    "            cur_tgt = conn_tgt.cursor()\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Ensure target table exists\n",
    "            # ------------------------------------------------------------------\n",
    "            cur_tgt.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_energy_trade (\n",
    "                trade_id BIGINT PRIMARY KEY,\n",
    "                time_id INT,\n",
    "                origin_country_id INT,\n",
    "                neighbor_country_id INT,\n",
    "                direction TEXT,\n",
    "                energy_value_gw FLOAT,\n",
    "                exchange_rate FLOAT,\n",
    "                value_eur FLOAT,\n",
    "                value_chf FLOAT,\n",
    "                UNIQUE(time_id, neighbor_country_id)\n",
    "            );\n",
    "            \"\"\")\n",
    "            conn_tgt.commit()\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 1: Load and aggregate hourly CBET energy data\n",
    "            # ------------------------------------------------------------------\n",
    "            sql_energy = f\"\"\"\n",
    "            SELECT\n",
    "                date_trunc('hour', timestamp) AS hour,\n",
    "                country,\n",
    "                SUM(value) AS energy_sum_gw\n",
    "            FROM tbl_energy_cbet_data\n",
    "            WHERE timestamp >= '{start_time}' AND timestamp < '{end_time}'\n",
    "            GROUP BY 1, 2\n",
    "            ORDER BY 1, 2;\n",
    "            \"\"\"\n",
    "            df_energy = pd.read_sql_query(sql_energy, conn_src)\n",
    "            df_energy = df_energy[df_energy['country'].str.strip().str.lower() != 'sum']\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 2: Load daily CHF to EUR exchange rates\n",
    "            # ------------------------------------------------------------------\n",
    "            sql_currency = \"\"\"\n",
    "            SELECT\n",
    "                DATE(timestamp) AS rate_date,\n",
    "                exchange_rate\n",
    "            FROM tbl_currency_data\n",
    "            WHERE source_currency='CHF' AND target_currency='EUR';\n",
    "            \"\"\"\n",
    "            df_curr = pd.read_sql_query(sql_currency, conn_src)\n",
    "            df_curr = df_curr.drop_duplicates(subset=['rate_date'])\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 3: Prepare energy dataframe with calculated fields\n",
    "            # ------------------------------------------------------------------\n",
    "            df_energy['direction'] = df_energy['energy_sum_gw'].apply(\n",
    "                lambda x: 'import' if x > 0 else ('export' if x < 0 else 'none')\n",
    "            )\n",
    "            df_energy['energy_value_gw'] = df_energy['energy_sum_gw'].abs()\n",
    "            df_energy['rate_date'] = pd.to_datetime(df_energy['hour']).dt.date\n",
    "\n",
    "            df_energy = df_energy.merge(df_curr, on='rate_date', how='left')\n",
    "\n",
    "            # Set fixed origin_country_id (Switzerland = 6)\n",
    "            df_energy['origin_country_id'] = 6\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 4: Map foreign keys using dimension tables\n",
    "            # ------------------------------------------------------------------\n",
    "\n",
    "            # Map country names to country IDs\n",
    "            sql_country = \"SELECT country_id, country_name_en FROM dim_countries;\"\n",
    "            df_country = pd.read_sql_query(sql_country, conn_tgt)\n",
    "            df_energy = df_energy.merge(df_country, left_on='country', right_on='country_name_en', how='left')\n",
    "            df_energy.rename(columns={'country_id': 'neighbor_country_id'}, inplace=True)\n",
    "\n",
    "            # Map timestamps to time IDs\n",
    "            sql_time = \"SELECT time_id, timestamp_utc FROM dim_time;\"\n",
    "            df_time = pd.read_sql_query(sql_time, conn_tgt)\n",
    "            df_time['timestamp_utc'] = pd.to_datetime(df_time['timestamp_utc'])\n",
    "            df_energy = df_energy.merge(df_time, left_on='hour', right_on='timestamp_utc', how='left')\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 5: Load hourly energy prices and calculate trade values\n",
    "            # ------------------------------------------------------------------\n",
    "            sql_price = f\"\"\"\n",
    "            SELECT\n",
    "                date_trunc('hour', timestamp) AS hour,\n",
    "                price\n",
    "            FROM tbl_energy_price_data\n",
    "            WHERE timestamp >= '{start_time}' AND timestamp < '{end_time}';\n",
    "            \"\"\"\n",
    "            df_price = pd.read_sql_query(sql_price, conn_src)\n",
    "            df_price = df_price.drop_duplicates(subset=['hour'])\n",
    "            df_price['hour'] = pd.to_datetime(df_price['hour']).dt.floor('H')\n",
    "            df_price.rename(columns={'price': 'price_eur_mwh'}, inplace=True)\n",
    "\n",
    "            df_energy = df_energy.merge(df_price[['hour', 'price_eur_mwh']], on='hour', how='left')\n",
    "\n",
    "            # Calculate monetary values\n",
    "            df_energy['value_eur'] = df_energy['price_eur_mwh'] * 1000 * df_energy['energy_value_gw']\n",
    "            df_energy['value_chf'] = df_energy['value_eur'] / df_energy['exchange_rate']\n",
    "            df_energy['value_eur'] = df_energy['value_eur'].round(2)\n",
    "            df_energy['value_chf'] = df_energy['value_chf'].round(2)\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 6: Generate trade_id and sort\n",
    "            # ------------------------------------------------------------------\n",
    "            cur_tgt.execute(\"SELECT COALESCE(MAX(trade_id), 0) FROM fact_energy_trade;\")\n",
    "            max_id = cur_tgt.fetchone()[0]\n",
    "            df_energy = df_energy.sort_values(['hour', 'neighbor_country_id'])\n",
    "            df_energy['trade_id'] = range(max_id + 1, max_id + 1 + len(df_energy))\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Step 7: Prepare and insert records into fact table\n",
    "            # ------------------------------------------------------------------\n",
    "            insert_sql = \"\"\"\n",
    "            INSERT INTO fact_energy_trade\n",
    "                (trade_id, time_id, origin_country_id, neighbor_country_id,\n",
    "                 direction, energy_value_gw, exchange_rate, value_eur, value_chf)\n",
    "            VALUES %s\n",
    "            \"\"\"\n",
    "\n",
    "            # Prepare clean records with native Python types\n",
    "            records = df_energy[['trade_id', 'time_id', 'origin_country_id',\n",
    "                                 'neighbor_country_id', 'direction',\n",
    "                                 'energy_value_gw', 'exchange_rate', 'value_eur', 'value_chf']].values.tolist()\n",
    "\n",
    "            clean_records = [\n",
    "                tuple(v.item() if hasattr(v, 'item') else v for v in row)\n",
    "                for row in records\n",
    "            ]\n",
    "\n",
    "            execute_values(cur_tgt, insert_sql, clean_records)\n",
    "            conn_tgt.commit()\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # Cleanup: Close DB connections\n",
    "            # ------------------------------------------------------------------\n",
    "            cur_tgt.close()\n",
    "            conn_src.close()\n",
    "            conn_tgt.close()\n",
    "\n",
    "        return {'status': 'success', 'rows_inserted': len(clean_records)}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps(f'Error: {str(e)}')\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
