{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ec42c",
   "metadata": {},
   "source": [
    "# Engery Charts API\n",
    "*Insert Lambda Function Name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745bff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b99a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "\n",
    "# Get Environment Variables\n",
    "endpoint = os.getenv('ENDPOINT_DL')\n",
    "db_name = os.getenv('DB_NAME_DL')\n",
    "username = os.getenv('USERNAME_DL')\n",
    "password = os.getenv('PASSWORD_DL')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615fa9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to DB & Bucket...\n",
      "Connection to DB successful.\n",
      "Connection to Bucket successful.\n",
      "CBET-Data successfully implemented.\n",
      "Data uploaded and connection closed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'statusCode': 200, 'body': '\"Inport succesfully\"'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def upload_dataframe_to_db(df, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Replace invalid characters in column names and convert index to a column\n",
    "    df = df.copy()\n",
    "    df.index.name = 'timestamp'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "    \n",
    "    # Generate CREATE TABLE query\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'unit' else 'float')}\"\n",
    "        for col in df.columns\n",
    "    ])\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    # INSERT query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "    \n",
    "    # Convert NaN to None for SQL compatibility\n",
    "    values = [tuple(None if pd.isna(x) else x for x in row) for row in df.values]\n",
    "    cur.executemany(insert_query, values)\n",
    "    \n",
    "    cur.close()\n",
    "\n",
    "def upload_dataframe_to_bucket(df, foldername,s3 ,bucket_name):\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = foldername + '/' + foldername + '_' + today + '.json'  # Key = path in the bucket\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def fetch_cbet_data(date: str) -> dict:\n",
    "    base_url = \"https://api.energy-charts.info/cbet\"\n",
    "    params = {\n",
    "        \"country\": \"ch\",\n",
    "        \"start\": date\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error during requesting CBET-Data for {date}: {response.status_code}\")\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def extract_cbet_data(json_data: dict) -> pd.DataFrame:\n",
    "    idx = pd.to_datetime(json_data[\"unix_seconds\"], unit=\"s\", utc=True)\n",
    "    idx.name = \"timestamp\"\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for country in json_data.get(\"countries\", []):\n",
    "        values = country.get(\"data\", [])\n",
    "        if len(values) != len(idx):\n",
    "            # length guard â€“ optional but helpful for debugging mismatches\n",
    "            raise ValueError(\n",
    "                f\"Length mismatch for {country.get('name')}: \"\n",
    "                f\"{len(values)} values vs {len(idx)} timestamps\"\n",
    "            )\n",
    "        df = pd.DataFrame(\n",
    "            {\"value\": values, \"country\": country.get(\"name\")},\n",
    "            index=idx\n",
    "        )\n",
    "        frames.append(df)\n",
    "\n",
    "    return pd.concat(frames).reset_index()\n",
    "\n",
    "def insert_cbet_data_to_db(df_cbet, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    df_cbet = df_cbet.fillna(0)\n",
    "\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'country' else 'float')}\"\n",
    "        for col in df_cbet.columns\n",
    "        ])\n",
    "\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (timestamp, country, value)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    data_tuples = [\n",
    "        (row['timestamp'], row['country'], row['value'])\n",
    "        for _, row in df_cbet.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"CBET-Data successfully implemented.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in implementing CBET-Data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def lambda_handler_historic():\n",
    "    date = dt.datetime.today().date()\n",
    "    start_date = str(date-dt.timedelta(days=1))\n",
    "\n",
    "    api_result_power = requests.get(f'https://api.energy-charts.info/public_power?country=ch&start={start_date}')\n",
    "    api_result_prices = requests.get(f'https://api.energy-charts.info/price?bzn=CH&start={start_date}')\n",
    "\n",
    "    api_response_power = api_result_power.json()\n",
    "    api_response_prices = api_result_prices.json()\n",
    "\n",
    "    cbet_json = fetch_cbet_data(start_date)\n",
    "\n",
    "    api_df_power = pd.DataFrame(api_response_power[\"production_types\"])\n",
    "    api_df_power_t = pd.DataFrame(api_df_power['data'].tolist()).T\n",
    "    api_df_power_t.columns = api_df_power['name']\n",
    "    api_df_power_t.index = pd.to_datetime(api_response_power[\"unix_seconds\"], unit='s', utc=True)\n",
    "\n",
    "    api_df_price = pd.DataFrame(api_response_prices[\"price\"])\n",
    "    api_df_price['Unit'] = api_response_prices[\"unit\"]\n",
    "    api_df_price.columns = ['Price', 'Unit']\n",
    "    api_df_price.index = pd.to_datetime(api_response_prices[\"unix_seconds\"], unit='s', utc=True)\n",
    "\n",
    "    api_df_cbet = extract_cbet_data(cbet_json)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client('s3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "                \n",
    "        # Upload DataFrames\n",
    "        upload_dataframe_to_db(api_df_power_t, \"tbl_energy_production_data\", conn)\n",
    "        upload_dataframe_to_db(api_df_price, \"tbl_energy_price_data\", conn)\n",
    "        insert_cbet_data_to_db(api_df_cbet, \"tbl_energy_cbet_data\", conn)\n",
    "        \n",
    "        \n",
    "\n",
    "        upload_dataframe_to_bucket(api_df_power_t, \"energy_production\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_price, \"energy_price\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_cbet, \"energy_cbet\",s3,bucket_name)\n",
    "\n",
    "        conn.close()\n",
    "        print(\"Data uploaded and connection closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(\"Inport succesfully\")\n",
    "    }\n",
    "\n",
    "lambda_handler_historic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045384e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
