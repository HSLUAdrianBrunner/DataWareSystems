{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLjx451v5-xg"
   },
   "source": [
    "# Code Documentation of Group2 | Datalake and Datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUO319FO6Xnk"
   },
   "source": [
    "## Lamdba Functions for API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6UHp_Zh6sfj"
   },
   "source": [
    "### group2_currency_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rE1GRu4j6YOw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Get Environment Variables\n",
    "\n",
    "endpoint = os.getenv('ENDPOINT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "username = os.getenv('USERNAME')\n",
    "password = os.getenv('PASSWORD')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n",
    "\n",
    "\n",
    "def upload_dataframe_to_db(df, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Replace invalid characters in column names and convert index to a column\n",
    "    df = df.copy()\n",
    "    df.index.name = 'timestamp'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "    \n",
    "    # Generate CREATE TABLE query\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col in ['unit', 'country'] else 'float')}\"\n",
    "        for col in df.columns\n",
    "    ])\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    # INSERT query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "    \n",
    "    # Convert NaN to None for SQL compatibility\n",
    "    values = [tuple(None if pd.isna(x) else x for x in row) for row in df.values]\n",
    "    cur.executemany(insert_query, values)\n",
    "    \n",
    "    cur.close()\n",
    "\n",
    "def upload_dataframe_to_bucket(df, foldername,s3 ,bucket_name):\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = foldername + '/' + foldername + '_' + today + '.json'  # Key = path in the bucket\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def fetch_cbet_data(date: str) -> dict:\n",
    "    base_url = \"https://api.energy-charts.info/cbet\"\n",
    "    params = {\n",
    "        \"country\": \"ch\",\n",
    "        \"start\": date\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error during requesting CBET-Data for {date}: {response.status_code}\")\n",
    "        return {}\n",
    "    \n",
    "\n",
    "def extract_cbet_data(json_data: dict) -> pd.DataFrame:\n",
    "    idx = (pd.to_datetime(json_data[\"unix_seconds\"], unit=\"s\", utc=True)\n",
    "             .tz_convert(\"Europe/Zurich\"))\n",
    "    idx.name = \"timestamp\"\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for country in json_data.get(\"countries\", []):\n",
    "        values = country.get(\"data\", [])\n",
    "        if len(values) != len(idx):\n",
    "            # length guard â€“ optional but helpful for debugging mismatches\n",
    "            raise ValueError(\n",
    "                f\"Length mismatch for {country.get('name')}: \"\n",
    "                f\"{len(values)} values vs {len(idx)} timestamps\"\n",
    "            )\n",
    "        df = pd.DataFrame(\n",
    "            {\"value\": values, \"country\": country.get(\"name\")},\n",
    "            index=idx\n",
    "        )\n",
    "        frames.append(df)\n",
    "\n",
    "    return pd.concat(frames).reset_index()\n",
    "\n",
    "def insert_cbet_data_to_db(df_cbet, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    df_cbet = df_cbet.fillna(0)\n",
    "\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col in ['unit', 'country'] else 'float')}\"\n",
    "        for col in df_cbet.columns\n",
    "        ])\n",
    "\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (timestamp, country, value)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    data_tuples = [\n",
    "        (row['timestamp'], row['country'], row['value'])\n",
    "        for _, row in df_cbet.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"CBET-Data successfully implemented.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in implementing CBET-Data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    date = dt.datetime.today().date()\n",
    "    start_date = str(date-dt.timedelta(days=1))\n",
    "\n",
    "    api_result_power = requests.get(f'https://api.energy-charts.info/public_power?country=ch&start={start_date}')\n",
    "    api_result_prices = requests.get(f'https://api.energy-charts.info/price?bzn=CH&start={start_date}')\n",
    "\n",
    "    api_response_power = api_result_power.json()\n",
    "    api_response_prices = api_result_prices.json()\n",
    "\n",
    "    cbet_json = fetch_cbet_data(start_date)\n",
    "\n",
    "    api_df_power = pd.DataFrame(api_response_power[\"production_types\"])\n",
    "    api_df_power_t = pd.DataFrame(api_df_power['data'].tolist()).T\n",
    "    api_df_power_t.columns = api_df_power['name']\n",
    "    api_df_power_t.index = pd.to_datetime(api_response_power[\"unix_seconds\"], unit='s', utc=True).tz_convert(\"Europe/Zurich\")\n",
    "\n",
    "    api_df_price = pd.DataFrame(api_response_prices[\"price\"])\n",
    "    api_df_price['Unit'] = api_response_prices[\"unit\"]\n",
    "    api_df_price.columns = ['Price', 'Unit']\n",
    "    api_df_price.index = pd.to_datetime(api_response_prices[\"unix_seconds\"], unit='s', utc=True).tz_convert(\"Europe/Zurich\")\n",
    "\n",
    "    api_df_cbet = extract_cbet_data(cbet_json)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client('s3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "                \n",
    "        # Upload DataFrames\n",
    "        upload_dataframe_to_db(api_df_power_t, \"tbl_energy_production_data\", conn)\n",
    "        upload_dataframe_to_db(api_df_price, \"tbl_energy_price_data\", conn)\n",
    "        insert_cbet_data_to_db(api_df_cbet, \"tbl_energy_cbet_data\", conn)\n",
    "        \n",
    "        \n",
    "\n",
    "        upload_dataframe_to_bucket(api_df_power_t, \"energy_production\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_price, \"energy_price\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_cbet, \"energy_cbet\",s3,bucket_name)\n",
    "\n",
    "        conn.close()\n",
    "        print(\"Data uploaded and connection closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(\"Inport succesfully\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3WvVZQo7uZ2"
   },
   "source": [
    "### group2_energy_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KdEhtA657ThA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import requests\n",
    "import psycopg2\n",
    "import os\n",
    "import boto3\n",
    "from psycopg2.extras import execute_values\n",
    "\n",
    "# Get Environment Variables\n",
    "\n",
    "endpoint = os.getenv('ENDPOINT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "username = os.getenv('USERNAME')\n",
    "password = os.getenv('PASSWORD')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n",
    "\n",
    "\n",
    "def upload_dataframe_to_db(df, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    # Replace invalid characters in column names and convert index to a column\n",
    "    df = df.copy()\n",
    "    df.index.name = 'timestamp'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col.lower().replace(\" \", \"_\").replace(\"-\", \"_\") for col in df.columns]\n",
    "\n",
    "    # Generate CREATE TABLE query\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'unit' else 'float')}\"\n",
    "        for col in df.columns\n",
    "    ])\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    # INSERT query\n",
    "    placeholders = ', '.join(['%s'] * len(df.columns))\n",
    "    insert_query = f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Convert NaN to None for SQL compatibility\n",
    "    values = [tuple(None if pd.isna(x) else x for x in row) for row in df.values]\n",
    "    cur.executemany(insert_query, values)\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "def upload_dataframe_to_bucket(df, foldername,s3 ,bucket_name):\n",
    "    today = str(dt.datetime.today().date())\n",
    "    key = foldername + '/' + foldername + '_' + today + '.json'  # Key = path in the bucket\n",
    "\n",
    "    data = df.reset_index().to_json(orient=\"records\", date_format=\"iso\")\n",
    "\n",
    "    s3.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=key,\n",
    "        Body=json.dumps(data),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "def fetch_cbet_data(date: str) -> dict:\n",
    "    base_url = \"https://api.energy-charts.info/cbet\"\n",
    "    params = {\n",
    "        \"country\": \"ch\",\n",
    "        \"start\": date\n",
    "    }\n",
    "    headers = {\n",
    "        'accept': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error during requesting CBET-Data for {date}: {response.status_code}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_cbet_data(json_data: dict) -> pd.DataFrame:\n",
    "    records = []\n",
    "    timestamps = json_data.get('unix_seconds', [])\n",
    "    for country in json_data.get('countries', []):\n",
    "        country_name = country.get('name')\n",
    "        values = country.get('data', [])\n",
    "\n",
    "        for ts, value in zip(timestamps, values):\n",
    "            dtime = dt.datetime.utcfromtimestamp(ts)\n",
    "            records.append({\n",
    "                'timestamp': dtime,\n",
    "                'country': country_name,\n",
    "                'value': value\n",
    "            })\n",
    "            df_records = pd.DataFrame(records)\n",
    "    return df_records\n",
    "\n",
    "def insert_cbet_data_to_db(df_cbet, table_name, conn):\n",
    "    cur = conn.cursor()\n",
    "    df_cbet = df_cbet.fillna(0)\n",
    "\n",
    "    column_defs = ', '.join([\n",
    "        f\"{col} {'timestamp' if col == 'timestamp' else ('text' if col == 'country' else 'float')}\"\n",
    "        for col in df_cbet.columns\n",
    "        ])\n",
    "\n",
    "    create_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
    "    cur.execute(create_query)\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "    INSERT INTO {table_name} (timestamp, country, value)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "    data_tuples = [\n",
    "        (row['timestamp'], row['country'], row['value'])\n",
    "        for _, row in df_cbet.iterrows()\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"CBET-Data successfully implemented.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in implementing CBET-Data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    date = dt.datetime.today().date()\n",
    "    start_date = str(date-dt.timedelta(days=1))\n",
    "\n",
    "    api_result_power = requests.get(f'https://api.energy-charts.info/public_power?country=ch&start={start_date}')\n",
    "    api_result_prices = requests.get(f'https://api.energy-charts.info/price?bzn=CH&start={start_date}')\n",
    "\n",
    "    api_response_power = api_result_power.json()\n",
    "    api_response_prices = api_result_prices.json()\n",
    "\n",
    "    cbet_json = fetch_cbet_data(start_date)\n",
    "\n",
    "    api_df_power = pd.DataFrame(api_response_power[\"production_types\"])\n",
    "    api_df_power_t = pd.DataFrame(api_df_power['data'].tolist()).T\n",
    "    api_df_power_t.columns = api_df_power['name']\n",
    "    api_df_power_t.index = pd.to_datetime(api_response_power[\"unix_seconds\"], unit='s')\n",
    "\n",
    "    api_df_price = pd.DataFrame(api_response_prices[\"price\"])\n",
    "    api_df_price['Unit'] = api_response_prices[\"unit\"]\n",
    "    api_df_price.columns = ['Price', 'Unit']\n",
    "    api_df_price.index = pd.to_datetime(api_response_prices[\"unix_seconds\"], unit='s')\n",
    "\n",
    "    api_df_cbet = extract_cbet_data(cbet_json)\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to DB & Bucket...\")\n",
    "        conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "        conn.set_session(autocommit=True)\n",
    "\n",
    "        print(\"Connection to DB successful.\")\n",
    "\n",
    "        s3 = boto3.client('s3',\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key\n",
    "            )\n",
    "\n",
    "        print(\"Connection to Bucket successful.\")\n",
    "\n",
    "        # Upload DataFrames\n",
    "        upload_dataframe_to_db(api_df_power_t, \"tbl_energy_production_data\", conn)\n",
    "        upload_dataframe_to_db(api_df_price, \"tbl_energy_price_data\", conn)\n",
    "        insert_cbet_data_to_db(api_df_cbet, \"tbl_energy_cbet_data\", conn)\n",
    "\n",
    "\n",
    "\n",
    "        upload_dataframe_to_bucket(api_df_power_t, \"energy_production\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_price, \"energy_price\",s3,bucket_name)\n",
    "        upload_dataframe_to_bucket(api_df_cbet, \"energy_cbet\",s3,bucket_name)\n",
    "\n",
    "        conn.close()\n",
    "        print(\"Data uploaded and connection closed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(\"Inport succesfully\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gpQwKYX7eVs"
   },
   "source": [
    "### group2_weather_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXJWAjAB7UGv"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Env Variable\n",
    "#db_user = os.getenv(\"DB_USER\")\n",
    "var_apikey = os.getenv(\"var_apikey\")\n",
    "var_database = os.getenv(\"var_database\")\n",
    "var_host = os.getenv(\"var_host\")\n",
    "var_password = os.getenv(\"var_password\")\n",
    "var_port = os.getenv(\"var_port\")\n",
    "var_user = os.getenv(\"var_user\")\n",
    "var_bucketname = os.getenv(\"var_bucketname\")\n",
    "var_aws_access_key_id = os.getenv(\"var_aws_access_key_id\")\n",
    "var_aws_secret_access_key = os.getenv(\"var_aws_secret_access_key\")\n",
    "\n",
    "\n",
    "# === PARAMETER ============================================\n",
    "DAYS_BACK = 1  # defining the amount of days from yesterday backwards\n",
    "\n",
    "# configuration parameter for db and bucket\n",
    "BUCKET_NAME = var_bucketname\n",
    "DB_CONFIG = {\n",
    "    \"host\": var_host,\n",
    "    \"port\": var_port,\n",
    "    \"database\": var_database,\n",
    "    \"user\": var_user,\n",
    "    \"password\": var_password\n",
    "}\n",
    "API_KEY = var_apikey\n",
    "AWS_ACCESS_KEY_ID = var_aws_access_key_id\n",
    "AWS_SECRET_ACCESS_KEY = var_aws_secret_access_key\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "# Definition of weather station location\n",
    "def get_city_data():\n",
    "    return  [\n",
    "    {\"City\": \"Aarau\", \"Latitude\": 47.392715, \"Longitude\": 8.044445, \"Canton\": \"Aargau\"},\n",
    "    {\"City\": \"Baden\", \"Latitude\": 47.473683, \"Longitude\": 8.308682, \"Canton\": \"Aargau\"},\n",
    "    {\"City\": \"Basel\", \"Latitude\": 47.558108, \"Longitude\": 7.587826, \"Canton\": \"Basel-City\"},\n",
    "    {\"City\": \"Bern\", \"Latitude\": 46.948474, \"Longitude\": 7.452175, \"Canton\": \"Bern\"},\n",
    "    {\"City\": \"Chur\", \"Latitude\": 46.854747, \"Longitude\": 9.526490, \"Canton\": \"Grisons\"},\n",
    "    {\"City\": \"Frauenfeld\", \"Latitude\": 47.556191, \"Longitude\": 8.896335, \"Canton\": \"Thurgau\"},\n",
    "    {\"City\": \"Genf\", \"Latitude\": 46.201756, \"Longitude\": 6.146601, \"Canton\": \"Geneva\"},\n",
    "    {\"City\": \"Lausanne\", \"Latitude\": 46.521827, \"Longitude\": 6.632702, \"Canton\": \"Vaud\"},\n",
    "    {\"City\": \"Lugano\", \"Latitude\": 46.005010, \"Longitude\": 8.952028, \"Canton\": \"Ticino\"},\n",
    "    {\"City\": \"Luzern\", \"Latitude\": 47.050545, \"Longitude\": 8.305468, \"Canton\": \"Lucerne\"},\n",
    "    {\"City\": \"Neuenburg\", \"Latitude\": 46.989583, \"Longitude\": 6.929264, \"Canton\": \"NeuchÃ¢tel\"},\n",
    "    {\"City\": \"Schaffhausen\", \"Latitude\": 47.696049, \"Longitude\": 8.634513, \"Canton\": \"Schaffhausen\"},\n",
    "    {\"City\": \"Sion\", \"Latitude\": 46.231175, \"Longitude\": 7.358879, \"Canton\": \"Wallis\"},\n",
    "    {\"City\": \"Solothurn\", \"Latitude\": 47.208135, \"Longitude\": 7.538405, \"Canton\": \"Solothurn\"},\n",
    "    {\"City\": \"St. Gallen\", \"Latitude\": 47.425059, \"Longitude\": 9.376588, \"Canton\": \"St. Gallen\"},\n",
    "    {\"City\": \"Winterthur\", \"Latitude\": 47.499172, \"Longitude\": 8.729150, \"Canton\": \"Zurich\"},\n",
    "    {\"City\": \"Zug\", \"Latitude\": 47.167990, \"Longitude\": 8.517365, \"Canton\": \"Zug\"},\n",
    "    {\"City\": \"ZÃ¼rich\", \"Latitude\": 47.374449, \"Longitude\": 8.541042, \"Canton\": \"Zurich\"}\n",
    "    ]\n",
    "\n",
    "# Sending request do weather API\n",
    "def fetch_weather_data(city, lat, lon, start, end, api_key):\n",
    "    url = f\"https://history.openweathermap.org/data/2.5/history/city?lat={lat}&lon={lon}&type=hour&start={start}&end={end}&appid={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error on {city}: {response.status_code}\")\n",
    "        return None\n",
    "    return response.json()\n",
    "\n",
    "def upload_to_s3(s3_client, city, date_str, data):\n",
    "    object_key = f\"weather_data/weatherdata_{city.lower()}_{date_str}.json\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=BUCKET_NAME,\n",
    "        Key=object_key,\n",
    "        Body=json.dumps(data, ensure_ascii=False).encode('utf-8'),\n",
    "        ContentType='application/json'\n",
    "    )\n",
    "\n",
    "# Extracting information from json into a list -> for dataframe\n",
    "def extract_weather_data(city, weather_json):\n",
    "    entries = []\n",
    "    for entry in weather_json.get(\"list\", []):\n",
    "        dt = datetime.utcfromtimestamp(entry[\"dt\"])\n",
    "        main = entry[\"main\"]\n",
    "        wind = entry.get(\"wind\", {})\n",
    "        weather = entry[\"weather\"][0]\n",
    "        rain = entry.get(\"rain\", {}).get(\"1h\", 0.0)\n",
    "        clouds = entry.get(\"clouds\", {}).get(\"all\", 0)\n",
    "        entries.append({\n",
    "            'City': city,\n",
    "            'datetime': dt,\n",
    "            'temp': main['temp'],\n",
    "            'pressure': main['pressure'],\n",
    "            'humidity': main['humidity'],\n",
    "            'temp_min': main['temp_min'],\n",
    "            'temp_max': main['temp_max'],\n",
    "            'wind_speed': wind.get('speed', 0.0),\n",
    "            'weather_main': weather['main'],\n",
    "            'weather_description': weather['description'],\n",
    "            'rain_1h': rain,\n",
    "            'clouds_all': clouds\n",
    "        })\n",
    "    return entries\n",
    "\n",
    "# Connecting to db\n",
    "def connect_to_db():\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "# Inserting data into db\n",
    "def insert_weather_data(conn, df_weather):\n",
    "    cur = conn.cursor()\n",
    "    df_weather = df_weather.fillna(0)\n",
    "    data_tuples = [\n",
    "        (\n",
    "            row['City'],\n",
    "            row['datetime'],\n",
    "            row['temp'],\n",
    "            row['pressure'],\n",
    "            row['humidity'],\n",
    "            row['temp_min'],\n",
    "            row['temp_max'],\n",
    "            row['wind_speed'],\n",
    "            row['weather_main'],\n",
    "            row['weather_description'],\n",
    "            row['rain_1h'],\n",
    "            row['clouds_all']\n",
    "        )\n",
    "        for _, row in df_weather.iterrows()\n",
    "    ]\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO tbl_weather_data (\n",
    "        city, datetime, temp, pressure, humidity,\n",
    "        temp_min, temp_max, wind_speed,\n",
    "        weather_main, weather_description, rain_1h, clouds_all\n",
    "    ) VALUES %s\n",
    "    \"\"\"\n",
    "    try:\n",
    "        execute_values(cur, insert_query, data_tuples)\n",
    "        conn.commit()\n",
    "        print(\"Data successfully inserted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during inserting data: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "# Running API request and data processing\n",
    "def lambda_handler(event, context):\n",
    "    df_coords = pd.DataFrame(get_city_data())\n",
    "    s3 = boto3.client('s3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    END_DATE = datetime.utcnow() - timedelta(days=1)  # yesterday\n",
    "    all_data = []\n",
    "    for days_ago in range(DAYS_BACK):\n",
    "        target_day = END_DATE - timedelta(days=days_ago)\n",
    "        start = int(datetime(target_day.year, target_day.month, target_day.day, 0, 0).timestamp())\n",
    "        end = int(datetime(target_day.year, target_day.month, target_day.day, 23, 59).timestamp())\n",
    "        date_str = target_day.strftime('%Y%m%d')\n",
    "        for _, row in df_coords.iterrows():\n",
    "            city = row[\"City\"]\n",
    "            lat = row[\"Latitude\"]\n",
    "            lon = row[\"Longitude\"]\n",
    "            weather_json = fetch_weather_data(city, lat, lon, start, end, API_KEY)\n",
    "            if not weather_json:\n",
    "                continue\n",
    "            upload_to_s3(s3, city, date_str, weather_json)\n",
    "            city_data = extract_weather_data(city, weather_json)\n",
    "            all_data.extend(city_data)\n",
    "\n",
    "\n",
    "    # Writing processed data into db\n",
    "    df_weather = pd.DataFrame(all_data)\n",
    "    if not df_weather.empty:\n",
    "        try:\n",
    "            conn = connect_to_db()\n",
    "            insert_weather_data(conn, df_weather)\n",
    "        except Exception as e:\n",
    "            print(f\"Connection error: {e}\")\n",
    "        finally:\n",
    "            if conn:\n",
    "                conn.close()\n",
    "    else:\n",
    "        print(\"No weather data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzKIpRVe8cYj"
   },
   "source": [
    "## Lamdba Functions for Data Processing in Datawarehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpJOe3Hf8qNP"
   },
   "source": [
    "### group2_dwh_fact_energy_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ichuJmA8a_Q"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import date, datetime, timedelta\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Load environment variables for both databases\n",
    "DB_CONFIG = {\n",
    "    \"host\": os.getenv(\"var_host\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password\")\n",
    "}\n",
    "\n",
    "DB_CONFIG2 = {\n",
    "    \"host\": os.getenv(\"var_host2\"),\n",
    "    \"port\": os.getenv(\"var_port\"),\n",
    "    \"database\": os.getenv(\"var_database2\"),\n",
    "    \"user\": os.getenv(\"var_user\"),\n",
    "    \"password\": os.getenv(\"var_password2\")\n",
    "}\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        # Establish database connections\n",
    "        conn1 = psycopg2.connect(**DB_CONFIG)\n",
    "        conn2 = psycopg2.connect(**DB_CONFIG2)\n",
    "        cur2 = conn2.cursor()\n",
    "\n",
    "        # Define the date for which to fetch data (2 days before today)\n",
    "        yesterday = date.today() - timedelta(days=2)\n",
    "\n",
    "        # Query data from the data lake\n",
    "        query_energy = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM tbl_energy_production_data\n",
    "            WHERE timestamp >= '{yesterday}'::date\n",
    "            AND timestamp < '{yesterday + timedelta(days=1)}'::date\n",
    "        \"\"\"\n",
    "\n",
    "        df_energy = pd.read_sql_query(query_energy, conn1)\n",
    "\n",
    "        # Load dimension tables from the data warehouse\n",
    "        df_locations = pd.read_sql_query(\"SELECT * FROM dim_locations\", conn2)\n",
    "        df_countries = pd.read_sql_query(\"SELECT * FROM dim_countries\", conn2)\n",
    "        df_time = pd.read_sql_query(\"SELECT * FROM dim_time\", conn2)\n",
    "\n",
    "        # Merge with time dimension to get time_id\n",
    "        df_merged = df_energy.merge(\n",
    "            df_time[['time_id', 'timestamp_utc']],\n",
    "            left_on='timestamp',\n",
    "            right_on='timestamp_utc',\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Select and rename relevant columns\n",
    "        df_fact = df_merged[[\n",
    "            'time_id', 'timestamp', 'solar', 'wind_onshore', 'load',\n",
    "            'renewable_share_of_load', 'residual_load'\n",
    "        ]].copy()\n",
    "        df_fact.rename(columns={\n",
    "            'solar': 'solar_output',\n",
    "            'wind_onshore': 'wind_output'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Add country_id for Switzerland\n",
    "        switzerland_id = df_countries.loc[\n",
    "            df_countries['iso_code'] == 'CH', 'country_id'\n",
    "        ].values[0]\n",
    "        df_fact.insert(\n",
    "            loc=df_fact.columns.get_loc('time_id') + 1,\n",
    "            column='country_id',\n",
    "            value=switzerland_id\n",
    "        )\n",
    "\n",
    "        # Fetch existing time_ids from the target table to avoid duplicates\n",
    "        cur2.execute(\"SELECT time_id FROM fact_energy_production\")\n",
    "        existing_time_ids = {row[0] for row in cur2.fetchall()}\n",
    "\n",
    "        # Keep only records with time_ids not already in the table\n",
    "        df_fact = df_fact[~df_fact['time_id'].isin(existing_time_ids)]\n",
    "\n",
    "        # If no new data to insert, exit early\n",
    "        if df_fact.empty:\n",
    "            cur2.close()\n",
    "            conn1.close()\n",
    "            conn2.close()\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps('No new records to insert.')\n",
    "            }\n",
    "\n",
    "        # Get the current maximum production_id to generate unique keys\n",
    "        cur2.execute(\"SELECT COALESCE(MAX(production_id), 0) FROM fact_energy_production\")\n",
    "        max_production_id = cur2.fetchone()[0]\n",
    "\n",
    "        # Add production_id as a running index starting from the max + 1\n",
    "        df_fact.insert(0, 'production_id', range(max_production_id + 1, max_production_id + 1 + len(df_fact)))\n",
    "\n",
    "        # Prepare columns and values for insertion\n",
    "        columns = [\n",
    "            \"production_id\", \"time_id\", \"country_id\", \"timestamp\", \"solar_output\",\n",
    "            \"wind_output\", \"load\", \"renewable_share_of_load\", \"residual_load\"\n",
    "        ]\n",
    "        values = [tuple(x) for x in df_fact[columns].to_numpy()]\n",
    "\n",
    "        # Create SQL INSERT statement\n",
    "        insert_sql = f\"\"\"\n",
    "        INSERT INTO fact_energy_production ({', '.join(columns)})\n",
    "        VALUES %s\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute batch insert using execute_values for efficiency\n",
    "        execute_values(cur2, insert_sql, values)\n",
    "\n",
    "        # Commit transaction and close connections\n",
    "        conn2.commit()\n",
    "        cur2.close()\n",
    "        conn1.close()\n",
    "        conn2.close()\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('New records successfully inserted.')\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps(f'Error during processing: {str(e)}')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pI5PRsW9IJM"
   },
   "source": [
    "### group2_dwh_fact_......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcPeBwUf9JBK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cUn8Wae9VnH"
   },
   "source": [
    "### group2_dwh_fact_...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwfIZwRd9XNO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6Xgxh-B9YJj"
   },
   "source": [
    "## SQL Queries for DataLake and Datawarehoue tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8-heXg3-fAk"
   },
   "source": [
    "### DataLake tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0LV1AfLAcRH"
   },
   "source": [
    "tbl_currency_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRRbxVoAAfVi"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_currency_data (\n",
    "\t\"timestamp\" timestamp NOT NULL,\n",
    "\tsource_currency varchar(10) NOT NULL,\n",
    "\ttarget_currency varchar(10) NOT NULL,\n",
    "\texchange_rate float8 NULL,\n",
    "\tCONSTRAINT tbl_currency_data_pkey PRIMARY KEY (\"timestamp\", source_currency, target_currency)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbpHaDJrAjec"
   },
   "source": [
    "tbl_energy_cbet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUuXq7zOArkJ"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_cbet_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tcountry float8 NULL,\n",
    "\tvalue float8 NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oP6u8KJVA-Op"
   },
   "source": [
    "tbl_energy_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfR7xI6VAyLq"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_price_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tprice float8 NULL,\n",
    "\tunit text NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRbKTD0oAtZp"
   },
   "source": [
    "tbl_energy_production_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WzIwVwGMA2CH"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_energy_production_data (\n",
    "\t\"timestamp\" timestamp NULL,\n",
    "\tcross_border_electricity_trading float8 NULL,\n",
    "\tnuclear float8 NULL,\n",
    "\thydro_run_of_river float8 NULL,\n",
    "\thydro_water_reservoir float8 NULL,\n",
    "\thydro_pumped_storage float8 NULL,\n",
    "\t\"others\" float8 NULL,\n",
    "\twind_onshore float8 NULL,\n",
    "\tsolar float8 NULL,\n",
    "\t\"load\" float8 NULL,\n",
    "\tresidual_load float8 NULL,\n",
    "\trenewable_share_of_load float8 NULL,\n",
    "\trenewable_share_of_generation float8 NULL\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vyRQE-sAZLa"
   },
   "source": [
    "tbl_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8toZzvyf-tFw"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.tbl_weather_data (\n",
    "\tid serial4 NOT NULL,\n",
    "\tcity varchar(100) NULL,\n",
    "\tdatetime timestamp NULL,\n",
    "\t\"temp\" float8 NULL,\n",
    "\tpressure int4 NULL,\n",
    "\thumidity int4 NULL,\n",
    "\ttemp_min float8 NULL,\n",
    "\ttemp_max float8 NULL,\n",
    "\twind_speed float8 NULL,\n",
    "\tweather_main varchar(50) NULL,\n",
    "\tweather_description text NULL,\n",
    "\train_1h float8 NULL,\n",
    "\tclouds_all float8 NULL,\n",
    "\tCONSTRAINT tbl_weather_data_pkey PRIMARY KEY (id)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFPH_q7R-tdo"
   },
   "source": [
    "### Datawarehouse tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKeI_nTq-3Zi"
   },
   "source": [
    "dim_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8qBmVwX-4Wf"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE dim_countries (\n",
    "    country_id SERIAL PRIMARY KEY,\n",
    "    country_name_en VARCHAR(100),\n",
    "    country_name_de VARCHAR(100),\n",
    "    iso_code VARCHAR(5)\n",
    ");\n",
    "\n",
    "INSERT INTO dim_countries (country_name_en, country_name_de, iso_code)\n",
    "VALUES\n",
    "    ('Germany',      'Deutschland',   'DE'),\n",
    "    ('France',       'Frankreich',    'FR'),\n",
    "    ('Austria',      'Ã–sterreich',    'AT'),\n",
    "    ('Italy',        'Italien',       'IT'),\n",
    "    ('Liechtenstein','Liechtenstein', 'LI'),\n",
    "    ('Switzerland',    'Schweiz',     'CH');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2y75raLBVlu"
   },
   "source": [
    "dim_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G5wApLGOBR9S"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.dim_currency (\n",
    "\tcurrency_id serial4 NOT NULL,\n",
    "\tcurrency_code varchar(3) NOT NULL,\n",
    "\tCONSTRAINT dim_currency_pkey PRIMARY KEY (currency_id)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stsh12dz_A6V"
   },
   "source": [
    "dim_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMrQHLOw_Bnq"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE dim_locations (\n",
    "    location_id SERIAL PRIMARY KEY,\n",
    "    country VARCHAR(100) DEFAULT 'Switzerland',\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    lat DOUBLE PRECISION NOT NULL,\n",
    "    long DOUBLE PRECISION NOT NULL\n",
    ");\n",
    "\n",
    "-- Daten einfÃ¼gen\n",
    "INSERT INTO dim_locations (city, lat, long)\n",
    "VALUES\n",
    "    ('Aarau', 47.392715, 8.044445),\n",
    "    ('Baden', 47.473683, 8.308682),\n",
    "    ('Basel', 47.558108, 7.587826),\n",
    "    ('Bern', 46.948474, 7.452175),\n",
    "    ('Chur', 46.854747, 9.526490),\n",
    "    ('Frauenfeld', 47.556191, 8.896335),\n",
    "    ('Genf', 46.201756, 6.146601),\n",
    "    ('Lausanne', 46.521827, 6.632702),\n",
    "    ('Lugano', 46.005010, 8.952028),\n",
    "    ('Luzern', 47.050545, 8.305468),\n",
    "    ('Neuenburg', 46.989583, 6.929264),\n",
    "    ('Schaffhausen', 47.696049, 8.634513),\n",
    "    ('Sion', 46.231175, 7.358879),\n",
    "    ('Solothurn', 47.208135, 7.538405),\n",
    "    ('St. Gallen', 47.425059, 9.376588),\n",
    "    ('Winterthur', 47.499172, 8.729150),\n",
    "    ('Zug', 47.167990, 8.517365),\n",
    "    ('ZÃ¼rich', 47.374449, 8.541042);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whz-8X1qBgFc"
   },
   "source": [
    "dim_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJ6008UcBgx4"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE public.dim_time (\n",
    "\ttime_id serial4 NOT NULL,\n",
    "\ttimestamp_utc timestamp NOT NULL,\n",
    "\t\"date\" date NOT NULL,\n",
    "\t\"hour\" int4 NULL,\n",
    "\tday_of_week varchar(3) NULL,\n",
    "\t\"month\" varchar(3) NULL,\n",
    "\t\"year\" int4 NULL,\n",
    "\tCONSTRAINT dim_time_day_of_week_check CHECK (((day_of_week)::text = ANY ((ARRAY['Mon'::character varying, 'Tue'::character varying, 'Wed'::character varying, 'Thu'::character varying, 'Fri'::character varying, 'Sat'::character varying, 'Sun'::character varying])::text[]))),\n",
    "\tCONSTRAINT dim_time_hour_check CHECK (((hour >= 0) AND (hour <= 23))),\n",
    "\tCONSTRAINT dim_time_month_check CHECK (((month)::text = ANY ((ARRAY['Jan'::character varying, 'Feb'::character varying, 'Mar'::character varying, 'Apr'::character varying, 'May'::character varying, 'Jun'::character varying, 'Jul'::character varying, 'Aug'::character varying, 'Sep'::character varying, 'Oct'::character varying, 'Nov'::character varying, 'Dec'::character varying])::text[]))),\n",
    "\tCONSTRAINT dim_time_pkey PRIMARY KEY (time_id),\n",
    "\tCONSTRAINT dim_time_year_check CHECK ((year >= 1900))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX65dSxc9kcO"
   },
   "source": [
    "fact_energy_production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qv-Ah7s89ihb"
   },
   "outputs": [],
   "source": [
    "CREATE TABLE fact_energy_production (\n",
    "    production_id SERIAL PRIMARY KEY,\n",
    "    time_id INTEGER NOT NULL,\n",
    "    country_id INTEGER NOT NULL,\n",
    "    timestamp TIMESTAMP NOT NULL,\n",
    "    solar_output FLOAT,\n",
    "    wind_output FLOAT,\n",
    "    load FLOAT,\n",
    "    renewable_share_of_load FLOAT,\n",
    "    residual_load FLOAT\n",
    ");\n",
    "\n",
    "\n",
    "-- Foreign Keys\n",
    "ALTER TABLE fact_energy_production\n",
    "ADD CONSTRAINT fk_time\n",
    "FOREIGN KEY (time_id) REFERENCES dim_time(time_id);\n",
    "\n",
    "ALTER TABLE fact_energy_production\n",
    "ADD CONSTRAINT fk_country\n",
    "FOREIGN KEY (country_id) REFERENCES dim_countries(country_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-B7T8u6Bd0J"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoG0TQXpBVguzKLabkEQlT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
