{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from zoneinfo import ZoneInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7eb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "\n",
    "# Get general Environment Variables\n",
    "username = os.getenv('USERNAME_DL')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "bucket_name = os.getenv('BUCKET_NAME')\n",
    "\n",
    "# Get Datalake Environment Variables \n",
    "dl_endpoint = os.getenv('ENDPOINT_DL')\n",
    "dl_db_name = os.getenv('DB_NAME_DL')\n",
    "dl_password = os.getenv('PASSWORD_DL')\n",
    "\n",
    "# Get Warehouse Environment Variables\n",
    "wh_endpoint = os.getenv(\"ENDPOINT_WH\")\n",
    "wh_db_name = os.getenv(\"DB_NAME_WH\")\n",
    "wh_password = os.getenv(\"PASSWORD_WH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9161785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Datalake \n",
      "Connection to Datalake successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_3104\\104975373.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sunshine data downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_3104\\104975373.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data downloaded.\n",
      "Data successfully processed.\n",
      "Connecting to WareHouse \n",
      "Connection to WareHouse successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_3104\\104975373.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_3104\\104975373.py:13: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'fact_weather' created successfully.\n",
      "Table 'fact_weather' created and data uploaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[        temp  wind_speed  sunshine_minutes  location_id  time_id  weather_id\n",
       " 0        NaN         NaN               NaN          NaN        1           0\n",
       " 1        NaN         NaN               NaN          NaN        2           1\n",
       " 2        NaN         NaN               NaN          NaN        3           2\n",
       " 3        NaN         NaN               NaN          NaN        4           3\n",
       " 4        NaN         NaN               NaN          NaN        5           4\n",
       " ...      ...         ...               ...          ...      ...         ...\n",
       " 166963   NaN         NaN               NaN          NaN    13124      166963\n",
       " 166964   NaN         NaN               NaN          NaN    13125      166964\n",
       " 166965   NaN         NaN               NaN          NaN    13126      166965\n",
       " 166966   NaN         NaN               NaN          NaN    13127      166966\n",
       " 166967   NaN         NaN               NaN          NaN    13128      166967\n",
       " \n",
       " [166968 rows x 6 columns],\n",
       " <connection object at 0x00000165026F5030; dsn: 'user=postgres password=xxx dbname=group2dbdatawarehouse host=group2dbdatawarehouse.cqbkg4qk84i1.us-east-1.rds.amazonaws.com', closed: 0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def connect_to_db(endpoint, db_name, username, password):\n",
    "    conn = psycopg2.connect(\n",
    "            host=endpoint,\n",
    "            dbname=db_name,\n",
    "            user=username,\n",
    "            password=password\n",
    "        )\n",
    "    conn.set_session(autocommit=True)\n",
    "    return conn\n",
    "\n",
    "def get_data_from_db(conn, table_name):\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_sunshine_percentage(df_sun):\n",
    "\n",
    "    def time_to_float(t):\n",
    "        if isinstance(t, str):\n",
    "            h, m, s = map(int, t.split(':'))\n",
    "        else:\n",
    "            h, m, s = t.hour, t.minute, t.second\n",
    "        return h + m/60 + s/3600\n",
    "\n",
    "    df = df_sun.copy()\n",
    "    df['sunrise_float'] = df['sunrise'].apply(time_to_float)\n",
    "    df['sunset_float'] = df['sunset'].apply(time_to_float)\n",
    "\n",
    "    for hour in range(24):\n",
    "        def hour_percentage(row):\n",
    "            start = row['sunrise_float']\n",
    "            end = row['sunset_float']\n",
    "            hour_start = hour\n",
    "            hour_end = hour + 1\n",
    "            # No sun during this hour\n",
    "            if end <= hour_start or start >= hour_end:\n",
    "                return 0.0\n",
    "            # Sun is up for part or all of this hour\n",
    "            overlap_start = max(start, hour_start)\n",
    "            overlap_end = min(end, hour_end)\n",
    "            return max(0.0, (overlap_end - overlap_start)) * 100\n",
    "\n",
    "        df[f'sunshine_pct_hour_{hour}'] = df.apply(hour_percentage, axis=1)\n",
    "\n",
    "    # Melt the DataFrame to have columns: city, date, hour, sunshine_pct_hour\n",
    "    df = df.melt(\n",
    "        id_vars=['city', 'date'],\n",
    "        value_vars=[f'sunshine_pct_hour_{hour}' for hour in range(24)],\n",
    "        var_name='hour',\n",
    "        value_name='sunshine_pct_hour'\n",
    "    )\n",
    "      \n",
    "    # Extract the hour as integer from the column name\n",
    "    df['hour'] = df['hour'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "    df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hour'], unit='h')\n",
    "    df = df.drop(['date', 'hour'], axis=1)  \n",
    "    return df\n",
    "\n",
    "def upload_to_db(conn, table_name, df):\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(f'DROP TABLE IF EXISTS {table_name};')\n",
    "        cur.execute(f'''\n",
    "            CREATE TABLE {table_name} (\n",
    "                weather_id BIGINT PRIMARY KEY,\n",
    "                time_id BIGINT,\n",
    "                location_id DOUBLE PRECISION,\n",
    "                temperature DOUBLE PRECISION,\n",
    "                wind_speed DOUBLE PRECISION,\n",
    "                sunshine_minutes DOUBLE PRECISION\n",
    "                \n",
    "            );\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "    # Insert data row by row\n",
    "    insert_query = f'''\n",
    "        INSERT INTO {table_name} (weather_id, time_id, location_id, temperature, wind_speed, sunshine_minutes)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    '''\n",
    "\n",
    "    data_tuples = df.where(pd.notnull(df), None).values.tolist()\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        for row in data_tuples:\n",
    "            cur.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created and data uploaded successfully.\")\n",
    "\n",
    "\n",
    "def lambda_handler():\n",
    "\n",
    "    try:\n",
    "        print(\"Connecting to Datalake \")\n",
    "        \n",
    "        dl_conn = connect_to_db(dl_endpoint, dl_db_name, username, dl_password)\n",
    "\n",
    "        print(\"Connection to Datalake successful.\")\n",
    "\n",
    "\n",
    "        df_sun = get_data_from_db(dl_conn, 'tbl_weather_sun_data')\n",
    "        df_sun = calculate_sunshine_percentage(df_sun)\n",
    "        print(\"Sunshine data downloaded.\")\n",
    "\n",
    "        df_weather = get_data_from_db(dl_conn, 'tbl_weather_data')\n",
    "        print(\"Weather data downloaded.\")\n",
    "\n",
    "        # Calculate the sunshine duration in minutes\n",
    "        df_ws = df_weather.merge(df_sun, on=['city', 'datetime'], how='outer')\n",
    "        df_ws['sunshine_minutes'] = (100- df_ws['clouds_all']) * df_ws['sunshine_pct_hour'] / 10000 * 60\n",
    "\n",
    "        # clean up the DataFrame\n",
    "        df_ws = df_ws.drop(['id','pressure','humidity','temp_min','temp_max','weather_main','weather_description','rain_1h','clouds_all','sunshine_pct_hour'], axis=1)\n",
    "\n",
    "        print(\"Data successfully processed.\")\n",
    "\n",
    "        # Connect to warehouse\n",
    "        print(\"Connecting to WareHouse \")\n",
    "        wh_conn = connect_to_db(wh_endpoint, wh_db_name, username, wh_password)\n",
    "\n",
    "        print(\"Connection to WareHouse successful.\")\n",
    "\n",
    "        # join with city data\n",
    "        df_city = get_data_from_db(wh_conn, 'dim_locations')\n",
    "        df_city_s = df_city[['location_id' ,'city']]\n",
    "        df_joined_wl = df_ws.merge(df_city_s, on=['city'], how='outer')\n",
    "        df_joined_wl = df_joined_wl.drop(['city'], axis=1)\n",
    "\n",
    "        #join with time data\n",
    "        df_time = get_data_from_db(wh_conn, 'dim_time')\n",
    "        df_time_s = df_time[['time_id', 'timestamp_utc']]\n",
    "        df_time_s.columns = ['time_id', 'datetime']\n",
    "        df_joined_wld = df_joined_wl.merge(df_time_s, on=['datetime'], how='outer')\n",
    "        df_joined_wld = df_joined_wld.drop(['datetime'], axis=1)\n",
    "        df_joined_wld['weather_id'] = df_joined_wld.index\n",
    "\n",
    "        upload_to_db(wh_conn, 'fact_weather', df_joined_wld[['weather_id', 'time_id', 'location_id', 'temp', 'wind_speed', 'sunshine_minutes']])\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    return [df_joined_wld, wh_conn]\n",
    "\n",
    "lambda_handler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
