{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e167ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d7eb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env', override=True)\n",
    "\n",
    "# Get general Environment Variables\n",
    "username = os.getenv('USERNAME_DL')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID_INTERNAL')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY_INTERNAL')\n",
    "\n",
    "# Get Datalake Environment Variables \n",
    "dl_endpoint = os.getenv('ENDPOINT_DL')\n",
    "dl_db_name = os.getenv('DB_NAME_DL')\n",
    "dl_password = os.getenv('PASSWORD_DL')\n",
    "\n",
    "# Get Warehouse Environment Variables\n",
    "wh_endpoint = os.getenv(\"ENDPOINT_WH\")\n",
    "wh_db_name = os.getenv(\"DB_NAME_WH\")\n",
    "wh_password = os.getenv(\"PASSWORD_WH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9161785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Datalake \n",
      "Connection to Datalake successful.\n",
      "Connecting to WareHouse \n",
      "Connection to WareHouse successful.\n",
      "Query Performed: SELECT max(time_id),max(weather_id) FROM fact_weather;\n",
      "Query Performed: SELECT timestamp_utc FROM dim_time WHERE time_id = 12141;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_23644\\2062124585.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last timestamp in warehouse:  2025-05-20 20:00:00\n",
      "Query Performed: SELECT * FROM tbl_weather_sun_data WHERE date >= '2025-05-20';\n",
      "Sunshine data downloaded.\n",
      "Query Performed: SELECT * FROM tbl_weather_data WHERE datetime > '2025-05-20 20:00:00';\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_23644\\2062124585.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n",
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_23644\\2062124585.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data downloaded.\n",
      "New weather data available.\n",
      "Data successfully processed.\n",
      "Query Performed: SELECT * FROM dim_locations;\n",
      "Query Performed: SELECT * FROM dim_time;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arbei\\AppData\\Local\\Temp\\ipykernel_23644\\2062124585.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'fact_weather' created successfully.\n",
      "Table 'fact_weather' created and data uploaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def connect_to_db(endpoint: str, db_name: str, username: str, password: str) -> psycopg2.extensions.connection:\n",
    "    \"\"\"\n",
    "    Establish a connection to a PostgreSQL database.\n",
    "    \"\"\"\n",
    "    conn = psycopg2.connect(\n",
    "        host=endpoint,\n",
    "        dbname=db_name,\n",
    "        user=username,\n",
    "        password=password\n",
    "    )\n",
    "    conn.set_session(autocommit=True)\n",
    "    return conn\n",
    "\n",
    "def get_data_from_db(\n",
    "    conn: psycopg2.extensions.connection,\n",
    "    table_name: str,\n",
    "    expression: str = '*',\n",
    "    condition: str = ''\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query data from a database table and return as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    query = f\"SELECT {expression} FROM {table_name}\"\n",
    "    if condition != '':\n",
    "        query += f\" WHERE {condition}\"\n",
    "    query += ';'\n",
    "    print(\"Query Performed: \" + query)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    return df\n",
    "\n",
    "def calculate_sunshine_percentage(df_sun: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the percentage of sunshine for each hour of the day.\n",
    "    \"\"\"\n",
    "\n",
    "    def time_to_float(t) -> float:\n",
    "        # Convert time string or datetime.time to float hours\n",
    "        if isinstance(t, str):\n",
    "            h, m, s = map(int, t.split(':'))\n",
    "        else:\n",
    "            h, m, s = t.hour, t.minute, t.second\n",
    "        return h + m/60 + s/3600\n",
    "\n",
    "    df = df_sun.copy()\n",
    "    df['sunrise_float'] = df['sunrise'].apply(time_to_float)\n",
    "    df['sunset_float'] = df['sunset'].apply(time_to_float)\n",
    "\n",
    "    for hour in range(24):\n",
    "        def hour_percentage(row) -> float:\n",
    "            start = row['sunrise_float']\n",
    "            end = row['sunset_float']\n",
    "            hour_start = hour\n",
    "            hour_end = hour + 1\n",
    "            # No sun during this hour\n",
    "            if end <= hour_start or start >= hour_end:\n",
    "                return 0.0\n",
    "            # Sun is up for part or all of this hour\n",
    "            overlap_start = max(start, hour_start)\n",
    "            overlap_end = min(end, hour_end)\n",
    "            return max(0.0, (overlap_end - overlap_start)) * 100\n",
    "\n",
    "        df[f'sunshine_pct_hour_{hour}'] = df.apply(hour_percentage, axis=1)\n",
    "\n",
    "    # Melt the DataFrame to have columns: city, date, hour, sunshine_pct_hour\n",
    "    df = df.melt(\n",
    "        id_vars=['city', 'date'],\n",
    "        value_vars=[f'sunshine_pct_hour_{hour}' for hour in range(24)],\n",
    "        var_name='hour',\n",
    "        value_name='sunshine_pct_hour'\n",
    "    )\n",
    "    # Extract the hour as integer from the column name\n",
    "    df['hour'] = df['hour'].str.extract(r'(\\d+)').astype(int)\n",
    "    df['datetime'] = pd.to_datetime(df['date']) + pd.to_timedelta(df['hour'], unit='h')\n",
    "    df = df.drop(['date', 'hour'], axis=1)\n",
    "    return df\n",
    "\n",
    "def upload_to_db(conn: psycopg2.extensions.connection, table_name: str, df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Upload a DataFrame to a database table.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        # Create table if it does not exist\n",
    "        cur.execute(f'''\n",
    "            CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                weather_id       INTEGER PRIMARY KEY,\n",
    "                time_id          INTEGER REFERENCES dim_time(time_id),\n",
    "                location_id      INTEGER REFERENCES dim_location(location_id),\n",
    "                wind_speed       FLOAT,\n",
    "                sunshine_minutes FLOAT\n",
    "            );\n",
    "        ''')\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "    # Insert data row by row\n",
    "    insert_query = f'''\n",
    "        INSERT INTO {table_name} (weather_id, time_id, location_id, wind_speed, sunshine_minutes)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    '''\n",
    "    data_tuples = df.where(pd.notnull(df), None).values.tolist()\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "        for row in data_tuples:\n",
    "            cur.execute(insert_query, row)\n",
    "        conn.commit()\n",
    "\n",
    "    print(f\"Table '{table_name}' created and data uploaded successfully.\")\n",
    "\n",
    "def lambda_handler():\n",
    "    \"\"\"\n",
    "    Main ETL handler function to process and upload weather data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to Datalake \")\n",
    "        dl_conn = connect_to_db(dl_endpoint, dl_db_name, username, dl_password)\n",
    "        print(\"Connection to Datalake successful.\")\n",
    "\n",
    "        # Connect to warehouse\n",
    "        print(\"Connecting to WareHouse \")\n",
    "        wh_conn = connect_to_db(wh_endpoint, wh_db_name, username, wh_password)\n",
    "        print(\"Connection to WareHouse successful.\")\n",
    "\n",
    "        # Get the current situation in the warehouse\n",
    "        df_init_fact = get_data_from_db(wh_conn, 'fact_weather', 'max(time_id),max(weather_id)')\n",
    "        df = get_data_from_db(wh_conn, 'dim_time', 'timestamp_utc', f'time_id = {df_init_fact.iloc[0,0]}')\n",
    "        print(\"Last timestamp in warehouse: \", df.iloc[0,0])\n",
    "\n",
    "        # Download sunshine data from datalake\n",
    "        df_sun = get_data_from_db(conn=dl_conn, table_name='tbl_weather_sun_data', condition= f\"date >= '{df.iloc[0,0].date()}'\")\n",
    "        df_sun = calculate_sunshine_percentage(df_sun)\n",
    "        print(\"Sunshine data downloaded.\")\n",
    "\n",
    "        # Download weather data from datalake\n",
    "        df_weather = get_data_from_db(conn=dl_conn, table_name='tbl_weather_data', condition= f\"datetime > '{df.iloc[0,0]}'\")\n",
    "        print(\"Weather data downloaded.\")\n",
    "\n",
    "        if df_weather.empty:\n",
    "            print(\"No new weather data available.\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"New weather data available.\")\n",
    "\n",
    "        # Calculate the sunshine duration in minutes\n",
    "        df_ws = df_weather.merge(df_sun, on=['city', 'datetime'], how='inner')\n",
    "        df_ws['sunshine_minutes'] = (100 - df_ws['clouds_all']) * df_ws['sunshine_pct_hour'] / 10000 * 60\n",
    "\n",
    "        # Clean up the DataFrame\n",
    "        df_ws = df_ws.drop(['id','pressure','humidity','temp_min','temp_max','temp','weather_main','weather_description','rain_1h','clouds_all','sunshine_pct_hour'], axis=1)\n",
    "        print(\"Data successfully processed.\")\n",
    "\n",
    "        # Join with city data\n",
    "        df_city = get_data_from_db(wh_conn, 'dim_locations')\n",
    "        df_city_s = df_city[['location_id', 'city']]\n",
    "        df_joined_wl = df_ws.merge(df_city_s, on=['city'], how='outer')\n",
    "        df_joined_wl = df_joined_wl.drop(['city'], axis=1)\n",
    "\n",
    "        # Join with time data\n",
    "        df_time = get_data_from_db(wh_conn, 'dim_time')\n",
    "        df_time_s = df_time[['time_id', 'timestamp_utc']]\n",
    "        df_time_s.columns = ['time_id', 'datetime']\n",
    "        df_joined_wld = df_joined_wl.merge(df_time_s, on=['datetime'], how='inner')\n",
    "        df_joined_wld = df_joined_wld.drop(['datetime'], axis=1)\n",
    "        df_joined_wld['weather_id'] = df_joined_wld.index + df_init_fact.iloc[0,1] + 1\n",
    "\n",
    "        # Upload data to database\n",
    "        upload_to_db(wh_conn, 'fact_weather', df_joined_wld[['weather_id', 'time_id', 'location_id', 'wind_speed', 'sunshine_minutes']])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n",
    "\n",
    "    return df_joined_wld\n",
    "\n",
    "df = lambda_handler()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
